\documentclass[english]{article}
%% -----------------------------
%% Préambule
%% -----------------------------
\input{../cheatsht-preamble-general-en.tex}

%% -----------------------------
%% Variable definition
%% -----------------------------
\def\cours{Modern Actuarial Statistics II}
\def\sigle{MAS-II}

%
% 	Save more space than default
%
\setlength{\abovedisplayskip}{-15pt}
\setlist{leftmargin=*}

%% -----------------------------
%% 	Colour setup for sections
%% -----------------------------
\def\SectionColor{cobalt}
\def\SubSectionColor{azure(colorwheel)}
\def\SubSubSectionColor{azure(colorwheel)}
%%%	depth
\setcounter{secnumdepth}{0}
%% -----------------------------

%% -----------------------------
%% 	Format part
%% -----------------------------
\titleformat
	{\part}			%	command
	[display]		%	shape
	{\normalfont\bfseries\filcenter}	%	format
	{\LARGE\thepart}	%	label
	{1ex}			%	sep
	{
		\titlerule[2pt]
		\vspace{2ex}%
		\LARGE
	}				%	before-code
	[
		\vspace{1ex}%
		{\titlerule[2pt]}
	]				%	after-code
\titlespacing{\part}
	{0pc}			%	left margin spacing
	{-5mm}			%	vertical space before title
	{1pc}			%	seperation between title and non-sectioning text
\renewcommand\thepart{\Alph{part}}
%% -----------------------------

%% -----------------------------
%% Color definitions
%% -----------------------------
\definecolor{indigo(web)}{rgb}{0.29, 0.0, 0.51}
\definecolor{cobalt}{rgb}{0.0, 0.28, 0.67}
\definecolor{azure(colorwheel)}{rgb}{0.0, 0.5, 1.0}
%% -----------------------------

%% -----------------------------
%% Variable definition
%% -----------------------------
%%
%% Matrix notation variable (bold style)
%%
\newcommand\cololine[2]{\colorlet{temp}{.}\color{#1}\bar{\color{temp}#2}\color{temp}}
\newcommand\colbar[2]{\colorlet{temp}{.}\color{#1}\bar{\color{temp}#2}\color{temp}}
\newcommand\cumlaut[2][black]{\stackon[.33ex]{#2}{\textcolor{#1}{\kern-.04ex.\kern-.2ex.}}}

%% -----------------------------
%% Tabular spacing
%% -----------------------------
%%
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut


\begin{document}

\begin{center}
	\textsc{\Large Contributeurs}\\[0.5cm] 
\end{center}
\input{../contributeurs/contrib-MASII}

%\begin{rappel_enhanced}[Motivation]
%
%\end{rappel_enhanced}



\newpage
\raggedcolumns
\begin{multicols*}{2}
\tableofcontents


\newpage
\part{Prerequisites}\label{part:prereq}
\section{Distributions}\label{sec:00Distributions}
\begin{rappel_enhanced}[Context]
We typically use 3 types of random variable to describe losses:

\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}m{6cm} | >{\columncolor{beaublue}}m{3cm}  |}
\hline
\hyperlink{0discrDistr}{\color{bleudefrance} Frequency} or number of losses	&	always discrete	\\\hline
\hyperlink{0sevDistr}{\color{bleudefrance} Severity} or amount of losses (payment)	&	usually continuous, can be discrete or mixed too	\\\hline
\hyperlink{0aggDistr}{\color{bleudefrance} Aggregate} or total loss from summing a number (Frequency) of Severity variables	&	same as the severity	\\\hline
\end{tabular}
\end{center}
\end{rappel_enhanced}

\subsection{Discrete Distributions}\label{subsec:0discrDistr}
\begin{rappel_enhanced}[Context]
Discrete random variables are usually counting (frequency) variables, meaning their possible values are $\{0, 1, 2, \dots\}$
\end{rappel_enhanced}

\begin{definitionNOHFILL}[\hypertarget{0pmf}{Probability Mass Function (PMF)}]
$N$ is a \textit{discrete random variable} if it has a \textbf{\textit{probability mass function}} $p_{k}$ such that \lfbox[formula]{$p_{k} = \Pr(N = k)$}
\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
\hline\rowcolor{airforceblue} 
\textcolor{white}{\textbf{Definition}}	&	\textcolor{white}{\textbf{Domain}}		&	\textcolor{white}{\textbf{Condition}}	\\\specialrule{0.1em}{0em}{0em} 
$p_{k} = \Pr(N = k)$	&	$p_{k} \in [0, 1]$	&	$\sum_{k} p_{k} = 1$\Tstrut\\\hline
\end{tabular}
\end{center}
\end{definitionNOHFILL}

\begin{definitionNOHFILLprop}[\hypertarget{0poissDistr}{Poisson Distribution}]
\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
\hline\rowcolor{airforceblue} 
\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em}
$N \sim \text{Poisson}(\lambda)$	&	$\lambda	>	0$	&	$n = 0, 1, 2, \dots$	\\\hline
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{| >{\columncolor{airforceblue}}m{2cm} | >{\columncolor{beaublue}}m{4cm}  |}
\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$\Pr(N = n)$}	&	 \[=\frac{\textrm{e}^{-\lambda} \lambda^{n}}{n!}\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$E[N]$}	&	 \[=\lambda\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$Var(N)$}	&	 \[=\lambda\]		\\\specialrule{0.1em}{0em}{0em}
\end{tabular}
\end{center}

\begin{itemize}
	\item	If $N_{1}$ and $N_{2}$ are \textit{independent} Poisson r.v., then $N_{1} + N_{2} \sim \text{Poisson}(\lambda_{1} + \lambda_{2})$.
	\item	The $\textrm{e}^{-\lambda}$ term makes the probabilities sum to $1$ as the Taylor series for $\textrm{e}^{\lambda}$ is $$\textrm{e}^{\lambda} = 1 + \lambda + \frac{\lambda^{2}}{2!} + \cdots + \frac{\lambda^{n}}{n!} + \cdots$$
\end{itemize}
\end{definitionNOHFILLprop}

\begin{definitionNOHFILLprop}[\hypertarget{0binDistr}{Binomial Distribution}]
\begin{rappel_enhanced}[Context]
A binomial r.v. $N$ has $m$ \textit{independent} trials each having a probability $q$ of a loss where $n$ is the total number of losses.
\end{rappel_enhanced}

\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
\hline\rowcolor{airforceblue} 
\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em}
$N \sim \text{Bin}(m, q)$	&	$q \in (0, 1); m \in \mathbb{N}$	&	$n = 0, 1, 2, \dots$	\\\hline
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{| >{\columncolor{airforceblue}}m{2cm} | >{\columncolor{beaublue}}m{4cm}  |}
\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$\Pr(N = n)$}	&	 \[= \binom{m}{n} q^{n} (1 - q)^{m - n} \]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$E[N]$}	&	 \[= mq \]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$Var(N)$}	&	 \[= mq \]		\\\specialrule{0.1em}{0em}{0em}
\end{tabular}
\end{center}

\begin{itemize}
	\item	If $N_{1}$ and $N_{2}$ are \textit{independent} binomial r.v. with the \textbf{\textit{same}} $q$ then $N_{1} + N_{2} \sim \text{Bin}(m_{1} + m_{2}, q)$.
	\item	The case where $m = 1$ corresponds to a \textit{\textbf{Bernoulli}} r.v.
\end{itemize}
\end{definitionNOHFILLprop}

\begin{definitionNOHFILLprop}[\hypertarget{0geoDistr}{Geometric Distribution}]
\begin{rappel_enhanced}[Context]
A geometric r.v. $N$ with mean $\beta$ can be obtained by setting $n$ as the number of years \textit{\textbf{before}} the \underline{first} loss. Given the geometric distribution is memoryless, each year \textit{independently} has a loss with probability $\displaystyle \underbrace{\Pr(N = 0)}_{\shortstack{probability of a\\ loss the first year}} = \frac{1}{1 + \beta}$.	
\end{rappel_enhanced}

\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
\hline\rowcolor{airforceblue} 
\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em}
$N \sim \text{Geo}(\beta)$	&	$\beta > 0$	&	$n = 0, 1, 2, \dots$	\\\hline
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{| >{\columncolor{airforceblue}}m{2cm} | >{\columncolor{beaublue}}m{4cm}  |}
\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$\Pr(N = n)$}	&	 \[=\left(\frac{\beta}{1 + \beta}\right)^{n} \frac{1}{1 + \beta}	\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$\Pr(N \geq n)$}	&	 \[=\left(\frac{\beta}{1 + \beta}\right)^{n}	\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$E[N]$}	&	 \[=\beta\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$Var(N)$}	&	 \[=\beta(1 + \beta)\]		\\\specialrule{0.1em}{0em}{0em}
\end{tabular}
\end{center}

\begin{itemize}
	\item	Like the \hyperlink{0expDist}{\color{bleudefrance} exponential distribution}, the geometric distribution is memoryless:
		\begin{align*}
		\Pr(N = d + n | N \geq d)
		&=	\Pr(N = n)	\\
		E[N - d | N \geq d]
		&=	E[N]
		\end{align*}
\end{itemize}
\end{definitionNOHFILLprop}

\columnbreak	
\begin{definitionNOHFILLprop}[Negative Binomial Distribution]
\begin{rappel_enhanced}[Context]
A negative binomial r.v. $N$ represents the number of years $n$ with no loss \textit{before} the $r^{\text{th}}$ year with a loss. We obtain a negative binomial r.v. $N \sim \text{NBin}(r, \beta)$ by summing $r$ iid geometric r.v., $N_{1}, N_{2}, \dots, N_{r}$, all with the same mean $\beta$.
\end{rappel_enhanced}

\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
\hline\rowcolor{airforceblue} 
\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em}
$N \sim \text{NBin}(\beta)$	&	$r, \beta > 0$	&	$n = 0, 1, 2, \dots$	\\\hline
\end{tabular}
\end{center}



\begin{center}
\begin{tabular}{| >{\columncolor{airforceblue}}m{2cm} | >{\columncolor{beaublue}}m{6cm}  |}
\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$\Pr(N = n)$}	&	 \[ = \binom{r + n - 1}{r - 1} \left(\frac{\beta}{1 + \beta}\right)^{n} \left(\frac{1}{1 + \beta}\right)^{r}	\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$\Pr(N \geq n)$}	&	 \[=\left(\frac{\beta}{1 + \beta}\right)^{n}	\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$E[N]$}			&	 \[=r\beta\]					\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$Var(N)$}			&	 \[=r\beta(1 + \beta)\]		\\\specialrule{0.1em}{0em}{0em}
\end{tabular}
\end{center}

\begin{itemize}
	\item	A \hyperlink{0geoDistr}{\color{bleudefrance} geometric} r.v. is a negative binomial r.v. with $r = 1$.
\end{itemize}
\end{definitionNOHFILLprop}



\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c   >{\columncolor{beaublue}}c  >{\columncolor{beaublue}}c  |}
\hline\rowcolor{airforceblue}
\textcolor{white}{\textbf{Distribution}}	&	\textcolor{white}{\textbf{Mean}}		&		&	\textcolor{white}{\textbf{Variance}}	\\\specialrule{0.1em}{0em}{0em}
Binomial				&	$mq$			&	$>$	&	$mq(1 - q)$			\\\hline
Poisson				&	$\lambda$	&	$=$	&	$\lambda$			\\\hline
Geometric			&	$\beta$		&	$<$	&	$\beta(1 + \beta)$	\\\hline
Negative Binomial	&	$r\beta$		&	$<$	&	$r\beta(1 + \beta)$	\\\hline
\end{tabular}
\end{center}



\subsection{Severity Distributions}\label{subsec:0sevDistr}

%\begin{definitionNOHFILL}[\hypertarget{0pdf}{Probability Density Function (PDF)}]
%$X$ is a \textit{continuous random variable} if it has a \textbf{\textit{probability density function}} $f(x)$ such that \lfbox[formula]{$f(x)$}
%\begin{center}
%\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
%\hline\rowcolor{airforceblue} 
%\textcolor{white}{\textbf{Definition}}	&	\textcolor{white}{\textbf{Domain}}		&	\textcolor{white}{\textbf{Condition}}	\\\specialrule{0.1em}{0em}{0em} 
%$f(x) = $	&	$f(x) \geq 0$	&	$\int_{\mathbb{R}} f(x)dx = 1$\Tstrut\\\hline
%\end{tabular}
%\end{center}
%\end{definitionNOHFILL}

\subsection{Joint Distributions}\label{subsec:0jointDistr}
\subsection{Conditional Distributions}\label{subsec:0condDistr}
\subsection{Aggregate Distributions}\label{subsec:0aggDistr}
\subsection{Normal, Uniform, Pareto, Exponential, and Gamma}\label{subsec:0gaExpNoDistr}
%\begin{definitionNOHFILLprop}[Normal Distribution]
%\begin{rappel_enhanced}[Contexte]
%La distribution Pareto est un mélange de deux distributions exponentielles originalement conçue pour étudier des distributions de revenus. 
%\end{rappel_enhanced}
%
%\begin{center}
%\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
%\hline\rowcolor{airforceblue} 
%\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em} 
%$X \sim \text{Pareto}(\alpha, \theta)$	&	$\alpha, \theta	>	0$	&	$x \geq 0$	\\\hline
%\end{tabular}
%\end{center}
%
%\begin{center}
%\begin{tabular}{| >{\columncolor{airforceblue}}m{1cm} | >{\columncolor{beaublue}}m{4cm}  |}
%\specialrule{0.1em}{0em}{0em}
%\textcolor{white}{$f(x)$}	&	 \[=	\frac{\alpha\theta^{\alpha}}{(x + \theta)^{\alpha + 1}}\]		\\\specialrule{0.1em}{0em}{0em}
%\textcolor{white}{$F(x)$}	&	 \[=1 -	\left(\frac{\theta}{x + \theta}\right)^{\alpha}\]		\\\specialrule{0.1em}{0em}{0em}
%\end{tabular}
%\end{center}
%
%\begin{itemize}
%	\item	Si $X \sim \text{Pareto}(\alpha, \theta)$ alors \lfbox[formula]{$Y	=	(X	-	d	|	X	>	d)	\sim \text{Pareto}(\alpha, \theta + d)$}.
%\end{itemize}
%\end{definitionNOHFILLprop}
%
%
%\begin{definitionNOHFILLprop}[Uniform Distribution]
%\begin{center}
%\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
%\hline\rowcolor{airforceblue} 
%\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em} 
%$X \sim \text{Beta}(a, b, \theta)$	&	$a, b	>	0 \text{ et } \theta \geq 0$	&	$x \in [0, \theta]$	\\\hline
%\end{tabular}
%\end{center}
%
%\begin{center}
%\begin{tabular}{| >{\columncolor{airforceblue}}m{1cm} | >{\columncolor{beaublue}}m{4cm}  |}
%\specialrule{0.1em}{0em}{0em}
%\textcolor{white}{$f(x)$}	&	 \[= \frac{\theta}{\text{B}(a, b)}	\bigg(\frac{x}{\theta}\bigg)^{a - 1} \bigg(1 - \frac{x}{\theta}\bigg)^{b - 1}\]		\\\specialrule{0.1em}{0em}{0em}
%\end{tabular}
%\end{center}
%
%\begin{itemize}
%	\item	$X	\sim \text{Beta}(a = 1, b = 1, \theta) \sim \text{Unif}(0, \theta)$.
%	\item	Si $X \sim \text{Unif}(a, b)$ alors  \lfbox[formula]{$(X	|	X	>	d)	\sim \text{Unif}(d, b)$} et \lfbox[formula]{$(X	-	d	|	X	>	d)	\sim \text{Unif}(0, b - d)$}.
%\end{itemize}
%\end{definitionNOHFILLprop}
%
%
%\begin{definitionNOHFILLprop}[Pareto Distribution]
%\begin{rappel_enhanced}[Contexte]
%La distribution Pareto est un mélange de deux distributions exponentielles originalement conçue pour étudier des distributions de revenus. 
%\end{rappel_enhanced}
%
%\begin{center}
%\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
%\hline\rowcolor{airforceblue} 
%\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em} 
%$X \sim \text{Pareto}(\alpha, \theta)$	&	$\alpha, \theta	>	0$	&	$x \geq 0$	\\\hline
%\end{tabular}
%\end{center}
%
%\begin{center}
%\begin{tabular}{| >{\columncolor{airforceblue}}m{1cm} | >{\columncolor{beaublue}}m{4cm}  |}
%\specialrule{0.1em}{0em}{0em}
%\textcolor{white}{$f(x)$}	&	 \[=	\frac{\alpha\theta^{\alpha}}{(x + \theta)^{\alpha + 1}}\]		\\\specialrule{0.1em}{0em}{0em}
%\textcolor{white}{$F(x)$}	&	 \[=1 -	\left(\frac{\theta}{x + \theta}\right)^{\alpha}\]		\\\specialrule{0.1em}{0em}{0em}
%\end{tabular}
%\end{center}
%
%\begin{itemize}
%	\item	Si $X \sim \text{Pareto}(\alpha, \theta)$ alors \lfbox[formula]{$Y	=	(X	-	d	|	X	>	d)	\sim \text{Pareto}(\alpha, \theta + d)$}.
%\end{itemize}
%\end{definitionNOHFILLprop}
%
%\hypertarget{0expDist}{Exponential Distribution}
%
%\begin{definitionNOHFILLprop}[Gamma Distribution]
%\begin{center}
%\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
%\hline\rowcolor{airforceblue} 
%\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em} 
%$X \sim \text{Gamma}(\alpha, \theta)$	&	$\alpha, \theta > 0$	&	$x \geq	0$	\\\hline
%\end{tabular}
%\end{center}
%
%\begin{center}
%\begin{tabular}{| >{\columncolor{airforceblue}}m{1cm} | >{\columncolor{beaublue}}m{4cm}  |}
%\specialrule{0.1em}{0em}{0em}
%\textcolor{white}{$f(x)$}	&	 \[= \frac{x^{\alpha - 1} \textrm{e}^{-x/\theta}}{\Gamma(\alpha)\theta^{\alpha}}\]		\\\specialrule{0.1em}{0em}{0em}
%\end{tabular}
%\end{center}
%
%\begin{itemize}
%	\item	On appelle $\theta$ la moyenne et $\lambda	=	\frac{1}{\theta}$ le paramètre de fréquence (" \textit{rate} ").
%	\item	Soit n v.a. indépendantes \lfbox[conditions]{$X_{i}	\sim \text{Gamma}(\alpha_{i}, \theta)$} alors \lfbox[formula]{$\sum_{i = 1}^{n} X_{i} \sim \text{Gamma}(\sum_{i = 1}^{n} \alpha_{i}, \theta)$}.
%	\item	Soit n v.a. indépendantes \lfbox[conditions]{$X_{i}	\sim \text{Exp}(\lambda_{i})$} alors \lfbox[formula]{$Y	=	\min(X_{1}, \dots, X_{n})	\sim	\text{Exp}(\frac{1}{\sum_{i = 1}^{n} \lambda_{i})}$}.
%	\item	Si $X \sim \text{Exp}(\theta)$ alors \lfbox[formula]{$(X	-	d	|	X	>	d)	\sim \text{Exp}(\theta)$}.
%\end{itemize}
%\end{definitionNOHFILLprop}

\lfbox[formula]{$\gamma(1/2) = \sqrt{\pi}$}

\section{Statistics}\label{sec:01Statistics}
\begin{definitionNOHFILL}[Mode]
\begin{rappel_enhanced}[Context]
The mode is the value that occurs the most often. A non-mathematical example of the concept is looking at the most used letter in the English alphabet. The letter E is the most used letter in the dictionary and as such is the mode of the English language.
\end{rappel_enhanced}

In mathematical terms, the mode is the point which maximises the \hyperlink{0pmf}{\color{bleudefrance} PMF}/\hyperlink{0pdf}{\color{bleudefrance} PDF}.

\bigskip

Finding the mode of a continuous r.v. can be done by calculating the derivative of the PDF and finding the point where it equals $0$.  If the distribution is
\begin{itemize}
	\item	\textbf{unimodal}, i.e. it has a hump, then \lfbox[formula]{$\text{mode} = x \text{ s.t. } f'(x) = 0$}.
	\item	\underline{strictly increasing} or \underline{decreasing}, the mode will be one of the 2 extremes.
		\begin{itemize}
		\item	For example, the exponential distribution is strictly decreasing and its mode is always $0$.
		\end{itemize}
\end{itemize}

\bigskip

For discrete variables, there are some ways to simplify it's calculation:
\begin{itemize}
	\item	Using the table function on the calculator and seeing where the probabilities peak.
	\item	Using the algebraic approach of looking at $p_{k} / p_{k - 1}$.
		\begin{itemize}
		\item	$p_{k} > p_{k - 1}$ iff $p_{k}/p_{k - 1} > 1$.
		\item	The mode is the largest $k$ s.t. $p_{k} > p_{k - 1}$.
		\end{itemize}
\end{itemize}

\paragraph{Note}	In the exam, it's best to use the calculator approach.
\end{definitionNOHFILL}


\newpage
\part{Introduction to Credibility}\label{part:cred}
\section{Basic Framework of Credibility}\label{sec:A1credBasics}
\begin{rappel_enhanced}[Context]
The \textbf{\textit{limitation fluctuation credibility}} approach, or \textbf{\textit{classical credibility}} approach, calculates an updated prediction ($U$) of the \textbf{loss measure} as a weighted ($Z$) average of recent claim experience ($D$) and a rate ($M$) specified in the manual. Thus, we calculate the \textit{premium} paid by the \textit{risk group} as \lfbox[formula]{$U = ZD + (1 - Z)M$}.
\end{rappel_enhanced}

\begin{distributions}[Notation]
\begin{description}
	\item[$M$]	Predicted loss based on the "\textit{\textbf{m}anual}".
	\item[$D$]	Observe\textbf{d} losses based on the recent experience of the risk group.
	\item[$Z$]	Weight assigned to the recent experience $D$ called the \textbf{\textit{credibility factor}} with \lfbox[conditions]{$Z \in [0, 1]$}. 
	\item[$U$]	\textbf{U}pdated prediction of the premium.
\end{description}
\end{distributions}

\begin{distributions}[Terminology]
\begin{description}
	\item[Risk group]	block of insurance policies, covered for a period of time upon payment of a \textit{premium}.
	\item[Claim frequency]	The number of claims denoted $N$.
	\item[Claim severity]	The amount of the $i^{\text{th}}$ claim denoted $X_{i}$.
	\item[Aggregate loss]	The total loss denoted $S$ where $S = X_{1} + X_{2} + \hdots + X_{N}$.
	\item[Pure premium]	The pure premium denoted $P$ where $P = S/E$ with $E$ denoting the number of exposure units.
\end{description}
\end{distributions}

\begin{definitionGENERAL}{Exam tips}[][ao(english)]
Typical questions about this involve being given 3 of $M, D, Z, \text{ and } U$ then finding the missing one.
\end{definitionGENERAL}


\begin{rappel_enhanced}[Context]
With $\min\{D, M\} \leq U \leq \max\{D, M\}$, we can see that the credibility factor determines the relative importance of the claim experience of the risk group $D$ relative to the manual rate $M$.

\bigskip

If $Z = 1$, we obtain \textit{\underline{\nameref{subsec:FullCred}}} where the predicted premium depends only on the data ($U = D$). It follows that with $Z < 1$, we obtain \textit{\underline{\nameref{subsec:PartialCred}}} as the weighted average of both $D$ and $M$.
\end{rappel_enhanced}


%\columnbreak
\subsection{Full Credibility}\label{subsec:FullCred}
\begin{rappel_enhanced}[Contexte]
The classical credibility approach determines the \textit{\textbf{minimum} data size} required for the experience data ($D$) to be given \textbf{\textit{full credibility}}. The minimum data size, or \textit{\textbf{standard for full credibility}}, depends on the \textbf{loss measure}.
\end{rappel_enhanced}


\subsubsection{Claim Frequency}
The claim frequency random variable $N$ has mean $\mu_{N}$ and variance $\sigma^{2}_{N}$. 

If we assume $N \approx \mathcal{N}(\mu_{N}, \sigma^{2}_{N})$, then the probability of observing claim frequency \textbf{within $k$ of the mean} is \lfbox[formula]{$\Pr(\mu_{N} - k\mu_{N} \leq N \leq \mu_{N} + k\mu_{N}) = 2 \Phi\left(\frac{k \mu_{N}}{\sigma_{N}}\right) - 1$}.

\bigskip

We often assume that the claim frequency \lfbox[conditions]{$N \sim \text{Pois}(\lambda_{N})$} and then apply the normal approximation to find the standard for full credibility for claim frequency \lfbox[formula]{$\lambda_{F}$}. First, we impose that the probability of the claim being with $k$ of the mean must be at least $1 - \alpha$. Then, we rewrite \lfbox[conditions]{$\frac{k \mu_{N}}{\sigma_{N}} = k\sqrt{\lambda_{N}}$} and set \lfbox[formula]{$\lambda_{N} \geq \left(\frac{z_{1 - \alpha/2}}{k}\right)^{2}$} where \lfbox[conditions]{$\lambda_{F} = \left(\frac{z_{1 - \alpha/2}}{k}\right)^{2}$}.


\subsubsection{Claim Severity}
We assume that the loss amounts $X_{1}, X_{2}, \dots, X_{N}$ are independent and identically distributed random variables with mean $\mu_{X}$ and variance $\sigma^{2}_{X}$. Full credibility is attributed to \lfbox[conditions]{$D = \bar{X}$} if \lfbox[conditions]{$2\Phi\left(\frac{k \mu_{X}}{\sigma_{N}/\sqrt{N}}\right) - 1 \geq 1 - \alpha$}. 

\bigskip

Similarly to claim frequency, we apply the normal approximation with \lfbox[conditions]{$\bar{X} \approx \mathcal{N}\left(\mu_{X}, \sigma^{2}_{X}/N\right)$}. Then, we find \lfbox[formula]{$N \geq \left(\frac{z_{1 - \alpha/2}}{k}\right)^{2} \cdot \left(\frac{\sigma_{X}}{\mu_{X}}\right)^{2} = \lambda_{F} CV_{X}^{2}$} where the \textbf{\textit{standard for full credibility for claim severity}} is $\lambda_{F}CV_{X}^{2}$.


\subsubsection{Aggregate Loss}
For the aggregate loss $S = X_{1} + X_{2} + \hdots + X_{N}$, we have \lfbox[formula]{$\mu_{S} = \mu_{N} \mu_{X}$} and \lfbox[formula]{$\sigma^{2}_{S} = \mu_{N} \sigma^{2}_{X} + \mu_{X}^{2} \sigma^{2}_{N}$}.

\bigskip

With the same normality assumptions for the Poisson distributed $N$, we find \lfbox[formula]{$\lambda_{N} \geq \left(\frac{z_{1 - \alpha/2}}{k}\right)^{2} \cdot \left(\frac{\mu_{X}^{2} + \sigma^{2}_{X}}{\mu_{X}^{2}}\right) = \lambda_{F} (1 + CV_{X}^{2})$} where the \textbf{\textit{standard for full credibility for claim severity}} is $\lambda_{F}(1 + CV_{X}^{2})$.

\paragraph{Note}	The conditions are the same for the \textit{\textbf{Pure Premium}} as for the aggregate loss.


%\columnbreak
\subsection{Partial Credibility}\label{subsec:PartialCred}
The \textbf{\textit{credibility factor}} for :
\begin{description}
	\item[Claim Frequency]	is \lfbox[formula]{$Z = \sqrt{\frac{\lambda_{N}}{\lambda_{F}}}$}.
	\item[Claim Severity]	is \lfbox[formula]{$Z = \sqrt{\frac{N}{\lambda_{F} CV_{X}^{2}}}$}.
	\item[Aggregate Loss and Pure Premium]	is \lfbox[formula]{$Z = \sqrt{\frac{\lambda_{N}}{\lambda_{F}(1 + CV_{X}^{2})}}$}
\end{description}


\newpage
\section{Bühlmann Credibility}\label{sec:ABuhl}
\begin{rappel_enhanced}[Context]
Buhlmann's approach, a.k.a. the greatest accuracy approach or the least squares approach, estimates the future loss measure $X_{n }$
\end{rappel_enhanced}
\subsection{Basic framework}

 

\subsection{Variance components}



\subsection{Credibility factors}
%Bûhlmann and Bûhlmann-Straub factors
%Estimate loss



\newpage
\section{Bayesian Credibility}\label{sec:ABayes}
\subsection{Basic framework}


\subsection{Premium}



\subsection{Conjugate distributions}

\subsection{Nonparametric empirical Bayes method}




\newpage
\part{Linear Mixed Models}\label{part:LMM}
\begin{rappel_enhanced}[Context]
What distinguishes a linear mixed model is that it may include both \textbf{fixed-effect parameters} and \textbf{random effects}. The mix of these gives the linear \textit{mixed} model its name. Fixed-effect parameters describes the relationships of the covariates to the dependant variable for an \textit{entire population}. Random effects are specific to clusters or subjects \textit{within a population}. Random effects are thus directly used in modelling the random variation in the dependant variable at \underline{different levels} of the data.

\bigskip

Fixed factors are categorical or classification variables for which all levels (conditions) that are of interest are included. Random factors can be thought of as being \textit{randomly sampled} from a population of levels being studied. The text gives as an example the Dental Veneer case study where if we specified the tooth being sampled, selected teeth would become a fixed factor. This would however limit inferences by teeth rather than generalizing to "teeth within a patient". 
\end{rappel_enhanced}

The case studies use 3 types of data:
\begin{description}
	\item[clustered]	The dependant variable is measured once per subject (unit of analysis), and the units are grouped into/nested within clusters of units. 
		\begin{itemize}
		\item	We can have data sets that are two-level (e.g. rat pup data set), three-level (e.g. classroom data), etc.
		\item	For MAS-II, we shouldn't have beyond three levels.
		\end{itemize}
	\item[repeated-measures]	The dependant variable is measured more than once (on the same unit of analysis) across levels of a repeated-measures factor(s). (e.g. time, measurement conditions, etc.)
	\item[longitudinal]	The dependant variable is measured at several points in time for each unit of analysis.
		\begin{itemize}
		\item	\textit{\textbf{Clustered longitudinal}} data combines features of both. (e.g. Dental Veneer data set).
		\item	Each unit is measured more than once, but those units of analysis are nested within clusters.
		\end{itemize}
\end{description}


\begin{rappel_enhanced}[Context]
These 3 are \textbf{\textit{hierarchical}} data sets as the observations can be placed into levels of a hierarchy in the data.

\bigskip

Generally:
\begin{description}
	\item[Level 1]	most detailed level; subjects, repeated measures on the same unit of analysis.
	\item[Level 2]	clusters of units, units of analysis.
	\item[Level 3]	clusters of clusters, clusters of units.
\end{description}

\bigskip

Levels are emphasized in the text because they help to conceptualize LMM as simple models defined at each level of the data hierarchy. 
\end{rappel_enhanced}

\section{General Theory}\label{sec:BGenTheory}
\subsection{Residual Variance Structures}
Diagonal
\begin{align*}
	R_{i}
	&=	\begin{bmatrix}
		\sigma^{2}	&	0	&	\dots	&	0	\\
		0	&	\sigma^{2}	&	\dots	&	0	\\
		\vdots	&	\vdots	&	\ddots	&	\vdots	\\
		0	&	0	&	\dots	&	\sigma^{2}	\\
		\end{bmatrix}
\end{align*}
\begin{itemize}
	\item	Assumes residuals from the same subject are \textit{independent}.
\end{itemize}

Compound Symmetry
\begin{align*}
	R_{i}
	&=	\begin{bmatrix}
		\sigma^{2} + \sigma_{1}	&	\sigma_{1}	&	\dots	&	\sigma_{1}	\\
		\sigma_{1}	&	\sigma^{2} + \sigma_{1}	&	\dots	&	\sigma_{1}	\\
		\vdots	&	\vdots	&	\ddots	&	\vdots	\\
		\sigma_{1}	&	\sigma_{1}	&	\dots	&	\sigma^{2} + \sigma_{1}	\\
		\end{bmatrix}
\end{align*}
\begin{itemize}
	\item	Assumes \textit{equal correlation} between observations from the same individual.
	\item	Good for clustered or repeated measures data.
\end{itemize}


First Order Auto-Regressive ($AR(1)$)
\begin{align*}
	R_{i}
	&=	\begin{bmatrix}
		\sigma^{2}	&	\sigma^{2}\rho	&	\dots	&	\sigma^{2}\rho^{n_{i}-1}	\\
		\sigma^{2}\rho	&	\sigma^{2}	&	\dots	&	\sigma^{2}\rho^{n_{i}-2}	\\
		\vdots	&	\vdots	&	\ddots	&	\vdots	\\
		\sigma^{2}\rho^{n_{i}-1}	&	\sigma^{2}\rho^{n_{i}-2}	&	\dots	&	\sigma^{2}	\\
		\end{bmatrix}
\end{align*}
\begin{itemize}
	\item	Good for longitudinal with \textit{equal time} between observations.
\end{itemize}


\subsection{Model Assumptions}
\begin{definitionGENERAL}{Fixed Effects}[\circled{1}{trueblue}]

\end{definitionGENERAL}

\begin{definitionGENERAL}{Random Effects}[\circled{2}{trueblue}]

\end{definitionGENERAL}



\columnbreak
\section{Algorithms}\label{sec:BAlgorithms}
\begin{definitionNOHFILL}[Expectation Maximization]

\end{definitionNOHFILL}


\begin{definitionNOHFILL}[Newton-Raphson]

\end{definitionNOHFILL}


\begin{definitionNOHFILL}[Fisher Scoring Algorithm]

\end{definitionNOHFILL}



\subsection{Troubleshooting}



\columnbreak
\section{Hypothesis Testing}\label{sec:BHypTesting}

\subsection{Likelihood Ratio Tests}\label{subsec:LRT}
Mixture of Chi Squares

REML


\subsection{Non-Likelihood Ratio Tests}
\begin{definitionNOHFILL}[$t$-test]

approximating df, not n-p
\end{definitionNOHFILL}


\begin{definitionNOHFILL}[$F$-test]
\begin{rappel_enhanced}[Context]
Degrees of freedom of the numerator correspond
\end{rappel_enhanced}

We get that the test statistic $t \approx F_{\text{num. df}, \text{den. df}}$ where the numerator df corresponds to the number of parameters being tested and the denominator degrees of freedom is obtained from R. 

\bigskip

The particularity of the $F$-test is that we must make adjustments for it due to:
\begin{enumerate}
	\item	Random Effects
	\item	Potential correlation between residuals
	\item	Estimate covariance matrix
\end{enumerate}

We have a few ways of approximating them:

\begin{definitionNOHFILLpropos}[Scatterwhite]

\begin{itemize}
	\item	Method used by R.
\end{itemize}
\end{definitionNOHFILLpropos}

\begin{definitionNOHFILLpropos}[Kenward-Rogers]

\begin{itemize}
	\item	Method used by SAS.
\end{itemize}
\end{definitionNOHFILLpropos}

\begin{definitionNOHFILLsub}[Type $I$]
Sequential
\end{definitionNOHFILLsub}

\begin{definitionNOHFILLsub}[Type $III$]
Conditional
\end{definitionNOHFILLsub}
\end{definitionNOHFILL}


Using tests:
\begin{enumerate}
	\item	Compute test statistic
		\begin{itemize}
		\item	$F$-statistic would be too hard to compute, would have to be provided.
		\item	For $t$-test, may just give components of the calculation and have us compute $t$ to the compare it to the CV.
		\item	For both tests, the number of df would have to be provided.
		\end{itemize}
	\item	Look up critical value table
	\item	Reject null / keep effects if test statistic > CV
\end{enumerate}

\subsubsection{Other tests}
Omnibus Wald Test (good)
	similar to $F$-test
	test statistic asymptotically $\chi^{2}$
	

Wald $z$-test (not good)
	only good asymptotically and breaks in some situations
	text recommends LRT instead	
	
	
	
\section{EBLUPS}

\subsection{Intra Correlation Coefficient}
\lfbox[formula]{$ICC_{\text{whatever}} = \frac{\text{variance in common}}{\text{total variance}}$}.

2 level model $ICC_{\text{group}} = \frac{\sigma^{2}_{\text{lvl 2}}}{\sigma^{2}_{\text{lvl 2}} + \sigma^{2}}$
3 level model
	$ICC_{\text{lvl 3 group}} = \frac{\sigma^{2}_{\text{lvl 3}}}{\sigma^{2}_{\text{lvl 3}} + \sigma^{2}_{\text{lvl 2}} + \sigma^{2}}$
	$ICC_{\text{lvl 2 group}} = \frac{\sigma^{2}_{\text{lvl 3}} + \sigma^{2}_{\text{lvl 2}}}{\sigma^{2}_{\text{lvl 3}} + \sigma^{2}_{\text{lvl 2}} + \sigma^{2}}$


\subsection{EBLUPS}
EBLUP
\begin{description}
	\item[E]	
	\item[B]	Best i.e. lowest variance among all such unbiased estimators
	\item[L]	Linear as functions of $\bm{y}_{i}$
	\item[U]	Unbiased with $\text{E}[\hat{\bm{u}}_{i}] = \bm{u}_{i}$
	\item[P]	
\end{description}

\begin{itemize}
	\item	Typically tedious to calculate so we use computers unless we calculate only for 1 random effect.
\end{itemize}


Use Buhlmann's formula where:
\begin{description}
	\item[$M$]	Average predicted value from the implied marginal model
	\item[$\bar{Y}$]	Average observed value from group
	\item[$\sigma^{2}_{HM}$]	$\text{Var}(u_{j}) = \sigma^{2}_{int}$
	\item[$\mu_{PV}$]	$\text{Var}(\varepsilon_{ij}) = \sigma^{2}$
\end{description}
Prediction is for $M + u_{j} = M + Z_{j} (\bar{Y} - M)$.



\section{Information Criteria}
\begin{rappel_enhanced}[Context]
When comparing 2 nested models, the more complex will be better than the simpler model. While the \textit{\underline{\nameref{subsec:LRT}}} checks if the simpler model is sufficient, it does not enable us to directly compare the 2 models. In addition, with the LRT we are limited to nested models. The AIC and BIC measures permit us to compare several models which don't have to be nested. They do so by adding a penalty to the likelihood for a model's complexity via the amount of parameters it has.

\bigskip

We wish to maximize the likelihood of our observations. As observed for the LRT, maximizing the likelihood is equivalent to minimizing the loglikelihood or a function thereof. Namely, $-2 \times \ell(\theta)$ (a.k.a. the \hl{deviance}, see \textit{\underline{\nameref{part:BAandMCMC}}}). In both cases, we add a penalty to the measure we wish to minimize.
\end{rappel_enhanced}

\begin{definitionNOHFILLsub}[Akaike Information Criteria (AIC)]
The AIC penalizes models which have more parameters by adding twice the number of estimated parameters $p$ in the model to twice the negative loglikelihood: \lfbox[formula]{$AIC = -2\ell(\theta) + 2p$}. 

\bigskip

We choose the model with the smallest AIC.
\end{definitionNOHFILLsub}

\begin{rappel_enhanced}[Context]
The disadvantage of the AIC lies in that for 2 nested models the probability of choosing the simpler model knowing it's the true model does not tend towards 1 when the number of observations increases towards infinity. We thus consider it an \textit{inconsistent} measure.

\bigskip

In comparison, the BIC \textbf{is} a \textit{consistent} measure given its parameters penalty is a function of the number of observations.

\bigskip

That being said, in both cases, the probability of rejecting the simpler model while the true model is somewhere in between tends towards 1.
\end{rappel_enhanced}


\begin{definitionNOHFILLsub}[Bayesian Information Criteria (BIC)]
The BIC penalizes more severely models which have more parameters given its penalty is a function of the number of observations $n$: \lfbox[formula]{$BIC = -2\ell(\theta) + \ln(n)p$}.
\end{definitionNOHFILLsub}

\bigskip

To better understand the difference between the AIC and BIC penalty, we can use log rules to rewrite the measures:
\begin{align*}
	AIC	
	&=	-2\ln|\mathcal{L}(\theta)| + 2p	\\
	&=	-2\ln|\mathcal{L}(\theta)| + \ln\left(\textrm{e}^{2p}\right)	\\
	&=	-\left[\ln|\mathcal{L}(\theta)^{2}| - \ln\left|\left(\textrm{e}^{p}\right)^{2}\right|\right]	\\
	&=	-\ln\left|\frac{\mathcal{L}(\theta)^{2}}{\left(\textrm{e}^{p}\right)^{2}}\right|	
\end{align*}

\begin{align*}
	BIC
	&=	-2\ln|\mathcal{L}(\theta)| + \ln|n|p	\\
	&=	-\left[\ln\left|\mathcal{L}(\theta)^{2}\right| - \ln\left|n^{p}\right|\right]	\\
	&=	-\ln\left|\frac{\mathcal{L}(\theta)^{2}}{n^{p}}\right|	
\end{align*}

\bigskip

\begin{rappel_enhanced}[Context]
There's no agreement on which is better for LMM and the text tends to build models piecewise, testing between steps with LRT. So, we probably won't use them much.

\bigskip

Fundamentally, the AIC tries to to find the model that best describes the data under the belief that there is no "correct" model. In contrast, the BIC tries to find the "correct" model under the belief that such a model exists. 

\bigskip

Intuitively, we may think we'd prefer the AIC given that it's typically unrealistic to believe there exists a "correct" model. However, some feel the BIC often gives better results. \textit{However}, part C on \textit{\underline{\nameref{part:BAandMCMC}}} has other information criterion that are more complicated but arguably better. 
\end{rappel_enhanced}


\bigskip

Notes:
\begin{itemize}
	\item	REML criterion at convergence is the deviance ($-2\ell(\theta)$).
		\begin{itemize}
		\item	Likelihoods are typically $<1$ given they're probability densities.
		\item	Thus, loglikelihoods are typically negative.
		\item	Thus a positive output suggests they already multiplied by $-2$.
		\end{itemize}
\end{itemize}



\section{Graphical Tests}
Not heavily tested.
	Case study will have some graphs and there will be some questions about case study which may need graphs interpretation
	
marginal residual
	residual leftover plugging in estimated fixed effects
	rarely used
	
conditional (textbook) / response (R) / raw (typical) residuals
	residual from estimate of everything
	In LMM, variance of residual $\varepsilon_{ij}$ can vary based on other factors
	So, still not residual we want

	
standardized / normalized residuals
	conditional residual / estimated SD for that residual
	almost always prefer standardized residual
	
important:
\begin{itemize}
	\item	Use residual plots for normality testing
	\item	raw data plots are useless; ignore them.
	\item	Standardized residuals adjust the data so we can tell if a residual is an outlier because it's from a high variance group or because it's really an outlier.
\end{itemize}


implied marginal model is LMM w/o random effects but with same variance structure (var Yij same for both)

marginal model is with just same variance for everything 

\newpage
\part{Bayesian Analysis and Markov Chain Monte Carlo}\label{part:BAandMCMC}
%%%%	labbels not affixing to the parts? but to previous section? investigate



\newpage
\part{Statistical Learning}\label{part:statLearn}
\section{K-Nearest Neighbors}\label{sec:KNN}


\newpage
\section{Decision Trees}
%	building, purpose, pruning
%	bagging, random forests, boosting


\newpage
\section{Principal Components Analysis (PCA)}\label{sec:PCA}
%	purpose and computations
%	interpretation of software outputs	


\newpage
\section{Clustering}
%	purpose and computations
%	interpretation of software outputs





\end{multicols*}
\end{document}
