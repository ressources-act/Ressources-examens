\documentclass[english]{article}
%% -----------------------------
%% Préambule
%% -----------------------------
\input{../cheatsht-preamble-general-en.tex}

%% -----------------------------
%% Variable definition
%% -----------------------------
\def\cours{Modern Actuarial Statistics II}
\def\sigle{MAS-II}

%
% 	Save more space than default
%
\setlength{\abovedisplayskip}{-15pt}
\setlist{leftmargin=*}

%% -----------------------------
%% 	Colour setup for sections
%% -----------------------------
\def\SectionColor{cobalt}
\def\SubSectionColor{azure(colorwheel)}
\def\SubSubSectionColor{azure(colorwheel)}
%%%	depth
\setcounter{secnumdepth}{0}
%% -----------------------------

%% -----------------------------
%% 	Format part
%% -----------------------------
\titleformat
	{\part}			%	command
	[display]		%	shape
	{\normalfont\bfseries\filcenter}	%	format
	{\LARGE\thepart}	%	label
	{1ex}			%	sep
	{
		\titlerule[2pt]
		\vspace{2ex}%
		\LARGE
	}				%	before-code
	[
		\vspace{1ex}%
		{\titlerule[2pt]}
	]				%	after-code
\titlespacing{\part}
	{0pc}			%	left margin spacing
	{-5mm}			%	vertical space before title
	{1pc}			%	seperation between title and non-sectioning text
\renewcommand\thepart{\Alph{part}}
%% -----------------------------

%% -----------------------------
%% Color definitions
%% -----------------------------
\definecolor{indigo(web)}{rgb}{0.29, 0.0, 0.51}
\definecolor{cobalt}{rgb}{0.0, 0.28, 0.67}
\definecolor{azure(colorwheel)}{rgb}{0.0, 0.5, 1.0}
%% -----------------------------

%% -----------------------------
%% Variable definition
%% -----------------------------
%%
%% Matrix notation variable (bold style)
%%
\newcommand\cololine[2]{\colorlet{temp}{.}\color{#1}\bar{\color{temp}#2}\color{temp}}
\newcommand\colbar[2]{\colorlet{temp}{.}\color{#1}\bar{\color{temp}#2}\color{temp}}
\newcommand\cumlaut[2][black]{\stackon[.33ex]{#2}{\textcolor{#1}{\kern-.04ex.\kern-.2ex.}}}

%% -----------------------------
%% Tabular spacing
%% -----------------------------
%%
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut


\begin{document}

\begin{center}
	\textsc{\Large Contributeurs}\\[0.5cm] 
\end{center}
\input{../contributeurs/contrib-MASII}

%\begin{rappel_enhanced}[Motivation]
%
%\end{rappel_enhanced}



\newpage
\raggedcolumns
\begin{multicols*}{2}
\tableofcontents


\newpage
\part{Prerequisites}\label{part:prereq}
\section{Distributions}\label{sec:00Distributions}
\begin{rappel_enhanced}[Context]
We typically use 3 types of random variable to describe losses:

\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}m{6cm} | >{\columncolor{beaublue}}m{3cm}  |}
\hline
\hyperlink{0discrDistr}{\color{bleudefrance} Frequency} or number of losses	&	always discrete	\\\hline
\hyperlink{0sevDistr}{\color{bleudefrance} Severity} or amount of losses (payment)	&	usually continuous, can be discrete or mixed too	\\\hline
\hyperlink{0aggDistr}{\color{bleudefrance} Aggregate} or total loss from summing a number (Frequency) of Severity variables	&	same as the severity	\\\hline
\end{tabular}
\end{center}
\end{rappel_enhanced}

\subsection{Discrete Distributions}\label{subsec:0discrDistr}
\begin{rappel_enhanced}[Context]
Discrete random variables are usually counting (frequency) variables, meaning their possible values are $\{0, 1, 2, \dots\}$
\end{rappel_enhanced}

\begin{definitionNOHFILL}[\hypertarget{0pmf}{Probability Mass Function (PMF)}]
$N$ is a \textit{discrete random variable} if it has a \textbf{\textit{probability mass function}} $p_{k}$ such that \lfbox[formula]{$p_{k} = \Pr(N = k)$}
\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
\hline\rowcolor{airforceblue} 
\textcolor{white}{\textbf{Definition}}	&	\textcolor{white}{\textbf{Domain}}		&	\textcolor{white}{\textbf{Condition}}	\\\specialrule{0.1em}{0em}{0em} 
$p_{k} = \Pr(N = k)$	&	$p_{k} \in [0, 1]$	&	$\sum_{k} p_{k} = 1$\Tstrut\\\hline
\end{tabular}
\end{center}
\end{definitionNOHFILL}

\begin{definitionNOHFILLprop}[\hypertarget{0poissDistr}{Poisson Distribution}]
\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
\hline\rowcolor{airforceblue} 
\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em}
$N \sim \text{Poisson}(\lambda)$	&	$\lambda	>	0$	&	$n = 0, 1, 2, \dots$	\\\hline
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{| >{\columncolor{airforceblue}}m{2cm} | >{\columncolor{beaublue}}m{4cm}  |}
\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$\Pr(N = n)$}	&	 \[=\frac{\textrm{e}^{-\lambda} \lambda^{n}}{n!}\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$E[N]$}	&	 \[=\lambda\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$Var(N)$}	&	 \[=\lambda\]		\\\specialrule{0.1em}{0em}{0em}
\end{tabular}
\end{center}

\begin{itemize}
	\item	If $N_{1}$ and $N_{2}$ are \textit{independent} Poisson r.v., then $N_{1} + N_{2} \sim \text{Poisson}(\lambda_{1} + \lambda_{2})$.
	\item	The $\textrm{e}^{-\lambda}$ term makes the probabilities sum to $1$ as the Taylor series for $\textrm{e}^{\lambda}$ is $$\textrm{e}^{\lambda} = 1 + \lambda + \frac{\lambda^{2}}{2!} + \cdots + \frac{\lambda^{n}}{n!} + \cdots$$
\end{itemize}
\end{definitionNOHFILLprop}

\begin{definitionNOHFILLprop}[\hypertarget{0binDistr}{Binomial Distribution}]
\begin{rappel_enhanced}[Context]
A binomial r.v. $N$ has $m$ \textit{independent} trials each having a probability $q$ of a loss where $n$ is the total number of losses.
\end{rappel_enhanced}

\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
\hline\rowcolor{airforceblue} 
\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em}
$N \sim \text{Bin}(m, q)$	&	$q \in (0, 1); m \in \mathbb{N}$	&	$n = 0, 1, 2, \dots$	\\\hline
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{| >{\columncolor{airforceblue}}m{2cm} | >{\columncolor{beaublue}}m{4cm}  |}
\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$\Pr(N = n)$}	&	 \[= \binom{m}{n} q^{n} (1 - q)^{m - n} \]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$E[N]$}	&	 \[= mq \]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$Var(N)$}	&	 \[= mq \]		\\\specialrule{0.1em}{0em}{0em}
\end{tabular}
\end{center}

\begin{itemize}
	\item	If $N_{1}$ and $N_{2}$ are \textit{independent} binomial r.v. with the \textbf{\textit{same}} $q$ then $N_{1} + N_{2} \sim \text{Bin}(m_{1} + m_{2}, q)$.
	\item	The case where $m = 1$ corresponds to a \textit{\textbf{Bernoulli}} r.v.
\end{itemize}
\end{definitionNOHFILLprop}

\begin{definitionNOHFILLprop}[\hypertarget{0geoDistr}{Geometric Distribution}]
\begin{rappel_enhanced}[Context]
A geometric r.v. $N$ with mean $\beta$ can be obtained by setting $n$ as the number of years \textit{\textbf{before}} the \underline{first} loss. Given the geometric distribution is memoryless, each year \textit{independently} has a loss with probability $\displaystyle \underbrace{\Pr(N = 0)}_{\shortstack{probability of a\\ loss the first year}} = \frac{1}{1 + \beta}$.	
\end{rappel_enhanced}

\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
\hline\rowcolor{airforceblue} 
\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em}
$N \sim \text{Geo}(\beta)$	&	$\beta > 0$	&	$n = 0, 1, 2, \dots$	\\\hline
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{| >{\columncolor{airforceblue}}m{2cm} | >{\columncolor{beaublue}}m{4cm}  |}
\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$\Pr(N = n)$}	&	 \[=\left(\frac{\beta}{1 + \beta}\right)^{n} \frac{1}{1 + \beta}	\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$\Pr(N \geq n)$}	&	 \[=\left(\frac{\beta}{1 + \beta}\right)^{n}	\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$E[N]$}	&	 \[=\beta\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$Var(N)$}	&	 \[=\beta(1 + \beta)\]		\\\specialrule{0.1em}{0em}{0em}
\end{tabular}
\end{center}

\begin{itemize}
	\item	Like the \hyperlink{0expDist}{\color{bleudefrance} exponential distribution}, the geometric distribution is memoryless:
		\begin{align*}
		\Pr(N = d + n | N \geq d)
		&=	\Pr(N = n)	\\
		E[N - d | N \geq d]
		&=	E[N]
		\end{align*}
\end{itemize}
\end{definitionNOHFILLprop}

\columnbreak	
\begin{definitionNOHFILLprop}[Negative Binomial Distribution]
\begin{rappel_enhanced}[Context]
A negative binomial r.v. $N$ represents the number of years $n$ with no loss \textit{before} the $r^{\text{th}}$ year with a loss. We obtain a negative binomial r.v. $N \sim \text{NBin}(r, \beta)$ by summing $r$ iid geometric r.v., $N_{1}, N_{2}, \dots, N_{r}$, all with the same mean $\beta$.
\end{rappel_enhanced}

\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
\hline\rowcolor{airforceblue} 
\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em}
$N \sim \text{NBin}(\beta)$	&	$r, \beta > 0$	&	$n = 0, 1, 2, \dots$	\\\hline
\end{tabular}
\end{center}



\begin{center}
\begin{tabular}{| >{\columncolor{airforceblue}}m{2cm} | >{\columncolor{beaublue}}m{6cm}  |}
\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$\Pr(N = n)$}	&	 \[ = \binom{r + n - 1}{r - 1} \left(\frac{\beta}{1 + \beta}\right)^{n} \left(\frac{1}{1 + \beta}\right)^{r}	\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$\Pr(N \geq n)$}	&	 \[=\left(\frac{\beta}{1 + \beta}\right)^{n}	\]		\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$E[N]$}			&	 \[=r\beta\]					\\\specialrule{0.1em}{0em}{0em}
\textcolor{white}{$Var(N)$}			&	 \[=r\beta(1 + \beta)\]		\\\specialrule{0.1em}{0em}{0em}
\end{tabular}
\end{center}

\begin{itemize}
	\item	A \hyperlink{0geoDistr}{\color{bleudefrance} geometric} r.v. is a negative binomial r.v. with $r = 1$.
\end{itemize}
\end{definitionNOHFILLprop}



\begin{center}
\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c   >{\columncolor{beaublue}}c  >{\columncolor{beaublue}}c  |}
\hline\rowcolor{airforceblue}
\textcolor{white}{\textbf{Distribution}}	&	\textcolor{white}{\textbf{Mean}}		&		&	\textcolor{white}{\textbf{Variance}}	\\\specialrule{0.1em}{0em}{0em}
Binomial				&	$mq$			&	$>$	&	$mq(1 - q)$			\\\hline
Poisson				&	$\lambda$	&	$=$	&	$\lambda$			\\\hline
Geometric			&	$\beta$		&	$<$	&	$\beta(1 + \beta)$	\\\hline
Negative Binomial	&	$r\beta$		&	$<$	&	$r\beta(1 + \beta)$	\\\hline
\end{tabular}
\end{center}



\subsection{Severity Distributions}\label{subsec:0sevDistr}

%\begin{definitionNOHFILL}[\hypertarget{0pdf}{Probability Density Function (PDF)}]
%$X$ is a \textit{continuous random variable} if it has a \textbf{\textit{probability density function}} $f(x)$ such that \lfbox[formula]{$f(x)$}
%\begin{center}
%\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
%\hline\rowcolor{airforceblue} 
%\textcolor{white}{\textbf{Definition}}	&	\textcolor{white}{\textbf{Domain}}		&	\textcolor{white}{\textbf{Condition}}	\\\specialrule{0.1em}{0em}{0em} 
%$f(x) = $	&	$f(x) \geq 0$	&	$\int_{\mathbb{R}} f(x)dx = 1$\Tstrut\\\hline
%\end{tabular}
%\end{center}
%\end{definitionNOHFILL}

\subsection{Joint Distributions}\label{subsec:0jointDistr}
\subsection{Conditional Distributions}\label{subsec:0condDistr}
\subsection{Aggregate Distributions}\label{subsec:0aggDistr}
\subsection{Normal, Uniform, Pareto, Exponential, and Gamma}\label{subsec:0gaExpNoDistr}
%\begin{definitionNOHFILLprop}[Normal Distribution]
%\begin{rappel_enhanced}[Contexte]
%La distribution Pareto est un mélange de deux distributions exponentielles originalement conçue pour étudier des distributions de revenus. 
%\end{rappel_enhanced}
%
%\begin{center}
%\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
%\hline\rowcolor{airforceblue} 
%\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em} 
%$X \sim \text{Pareto}(\alpha, \theta)$	&	$\alpha, \theta	>	0$	&	$x \geq 0$	\\\hline
%\end{tabular}
%\end{center}
%
%\begin{center}
%\begin{tabular}{| >{\columncolor{airforceblue}}m{1cm} | >{\columncolor{beaublue}}m{4cm}  |}
%\specialrule{0.1em}{0em}{0em}
%\textcolor{white}{$f(x)$}	&	 \[=	\frac{\alpha\theta^{\alpha}}{(x + \theta)^{\alpha + 1}}\]		\\\specialrule{0.1em}{0em}{0em}
%\textcolor{white}{$F(x)$}	&	 \[=1 -	\left(\frac{\theta}{x + \theta}\right)^{\alpha}\]		\\\specialrule{0.1em}{0em}{0em}
%\end{tabular}
%\end{center}
%
%\begin{itemize}
%	\item	Si $X \sim \text{Pareto}(\alpha, \theta)$ alors \lfbox[formula]{$Y	=	(X	-	d	|	X	>	d)	\sim \text{Pareto}(\alpha, \theta + d)$}.
%\end{itemize}
%\end{definitionNOHFILLprop}
%
%
%\begin{definitionNOHFILLprop}[Uniform Distribution]
%\begin{center}
%\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
%\hline\rowcolor{airforceblue} 
%\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em} 
%$X \sim \text{Beta}(a, b, \theta)$	&	$a, b	>	0 \text{ et } \theta \geq 0$	&	$x \in [0, \theta]$	\\\hline
%\end{tabular}
%\end{center}
%
%\begin{center}
%\begin{tabular}{| >{\columncolor{airforceblue}}m{1cm} | >{\columncolor{beaublue}}m{4cm}  |}
%\specialrule{0.1em}{0em}{0em}
%\textcolor{white}{$f(x)$}	&	 \[= \frac{\theta}{\text{B}(a, b)}	\bigg(\frac{x}{\theta}\bigg)^{a - 1} \bigg(1 - \frac{x}{\theta}\bigg)^{b - 1}\]		\\\specialrule{0.1em}{0em}{0em}
%\end{tabular}
%\end{center}
%
%\begin{itemize}
%	\item	$X	\sim \text{Beta}(a = 1, b = 1, \theta) \sim \text{Unif}(0, \theta)$.
%	\item	Si $X \sim \text{Unif}(a, b)$ alors  \lfbox[formula]{$(X	|	X	>	d)	\sim \text{Unif}(d, b)$} et \lfbox[formula]{$(X	-	d	|	X	>	d)	\sim \text{Unif}(0, b - d)$}.
%\end{itemize}
%\end{definitionNOHFILLprop}
%
%
%\begin{definitionNOHFILLprop}[Pareto Distribution]
%\begin{rappel_enhanced}[Contexte]
%La distribution Pareto est un mélange de deux distributions exponentielles originalement conçue pour étudier des distributions de revenus. 
%\end{rappel_enhanced}
%
%\begin{center}
%\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
%\hline\rowcolor{airforceblue} 
%\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em} 
%$X \sim \text{Pareto}(\alpha, \theta)$	&	$\alpha, \theta	>	0$	&	$x \geq 0$	\\\hline
%\end{tabular}
%\end{center}
%
%\begin{center}
%\begin{tabular}{| >{\columncolor{airforceblue}}m{1cm} | >{\columncolor{beaublue}}m{4cm}  |}
%\specialrule{0.1em}{0em}{0em}
%\textcolor{white}{$f(x)$}	&	 \[=	\frac{\alpha\theta^{\alpha}}{(x + \theta)^{\alpha + 1}}\]		\\\specialrule{0.1em}{0em}{0em}
%\textcolor{white}{$F(x)$}	&	 \[=1 -	\left(\frac{\theta}{x + \theta}\right)^{\alpha}\]		\\\specialrule{0.1em}{0em}{0em}
%\end{tabular}
%\end{center}
%
%\begin{itemize}
%	\item	Si $X \sim \text{Pareto}(\alpha, \theta)$ alors \lfbox[formula]{$Y	=	(X	-	d	|	X	>	d)	\sim \text{Pareto}(\alpha, \theta + d)$}.
%\end{itemize}
%\end{definitionNOHFILLprop}
%
%\hypertarget{0expDist}{Exponential Distribution}
%
%\begin{definitionNOHFILLprop}[Gamma Distribution]
%\begin{center}
%\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  | >{\columncolor{beaublue}}c  |}
%\hline\rowcolor{airforceblue} 
%\textcolor{white}{\textbf{Notation}}	&	\textcolor{white}{\textbf{Parameters}}		&	\textcolor{white}{\textbf{Domain}}	\\\specialrule{0.1em}{0em}{0em} 
%$X \sim \text{Gamma}(\alpha, \theta)$	&	$\alpha, \theta > 0$	&	$x \geq	0$	\\\hline
%\end{tabular}
%\end{center}
%
%\begin{center}
%\begin{tabular}{| >{\columncolor{airforceblue}}m{1cm} | >{\columncolor{beaublue}}m{4cm}  |}
%\specialrule{0.1em}{0em}{0em}
%\textcolor{white}{$f(x)$}	&	 \[= \frac{x^{\alpha - 1} \textrm{e}^{-x/\theta}}{\Gamma(\alpha)\theta^{\alpha}}\]		\\\specialrule{0.1em}{0em}{0em}
%\end{tabular}
%\end{center}
%
%\begin{itemize}
%	\item	On appelle $\theta$ la moyenne et $\lambda	=	\frac{1}{\theta}$ le paramètre de fréquence (" \textit{rate} ").
%	\item	Soit n v.a. indépendantes \lfbox[conditions]{$X_{i}	\sim \text{Gamma}(\alpha_{i}, \theta)$} alors \lfbox[formula]{$\sum_{i = 1}^{n} X_{i} \sim \text{Gamma}(\sum_{i = 1}^{n} \alpha_{i}, \theta)$}.
%	\item	Soit n v.a. indépendantes \lfbox[conditions]{$X_{i}	\sim \text{Exp}(\lambda_{i})$} alors \lfbox[formula]{$Y	=	\min(X_{1}, \dots, X_{n})	\sim	\text{Exp}(\frac{1}{\sum_{i = 1}^{n} \lambda_{i})}$}.
%	\item	Si $X \sim \text{Exp}(\theta)$ alors \lfbox[formula]{$(X	-	d	|	X	>	d)	\sim \text{Exp}(\theta)$}.
%\end{itemize}
%\end{definitionNOHFILLprop}

\lfbox[formula]{$\gamma(1/2) = \sqrt{\pi}$}

\section{Statistics}\label{sec:01Statistics}
\begin{definitionNOHFILL}[Mode]
\begin{rappel_enhanced}[Context]
The mode is the value that occurs the most often. A non-mathematical example of the concept is looking at the most used letter in the English alphabet. The letter E is the most used letter in the dictionary and as such is the mode of the English language.
\end{rappel_enhanced}

In mathematical terms, the mode is the point which maximises the \hyperlink{0pmf}{\color{bleudefrance} PMF}/\hyperlink{0pdf}{\color{bleudefrance} PDF}.

\bigskip

Finding the mode of a continuous r.v. can be done by calculating the derivative of the PDF and finding the point where it equals $0$.  If the distribution is
\begin{itemize}
	\item	\textbf{unimodal}, i.e. it has a hump, then \lfbox[formula]{$\text{mode} = x \text{ s.t. } f'(x) = 0$}.
	\item	\underline{strictly increasing} or \underline{decreasing}, the mode will be one of the 2 extremes.
		\begin{itemize}
		\item	For example, the exponential distribution is strictly decreasing and its mode is always $0$.
		\end{itemize}
\end{itemize}

\bigskip

For discrete variables, there are some ways to simplify it's calculation:
\begin{itemize}
	\item	Using the table function on the calculator and seeing where the probabilities peak.
	\item	Using the algebraic approach of looking at $p_{k} / p_{k - 1}$.
		\begin{itemize}
		\item	$p_{k} > p_{k - 1}$ iff $p_{k}/p_{k - 1} > 1$.
		\item	The mode is the largest $k$ s.t. $p_{k} > p_{k - 1}$.
		\end{itemize}
\end{itemize}

\paragraph{Note}	In the exam, it's best to use the calculator approach.
\end{definitionNOHFILL}


\newpage
\part{Introduction to Credibility}\label{part:cred}
\section{Basic Framework of Credibility}\label{sec:A1credBasics}
\begin{rappel_enhanced}[Context]
The \textbf{\textit{limitation fluctuation credibility}} approach, or \textbf{\textit{classical credibility}} approach, calculates an updated prediction ($U$) of the \textbf{loss measure} as a weighted ($Z$) average of recent claim experience ($D$) and a rate ($M$) specified in the manual. Thus, we calculate the \textit{premium} paid by the \textit{risk group} as \lfbox[formula]{$U = ZD + (1 - Z)M$}.
\end{rappel_enhanced}

\begin{distributions}[Notation]
\begin{description}
	\item[$M$]	Predicted loss based on the "\textit{\textbf{m}anual}".
	\item[$D$]	Observe\textbf{d} losses based on the recent experience of the risk group.
	\item[$Z$]	Weight assigned to the recent experience $D$ called the \textbf{\textit{credibility factor}} with \lfbox[conditions]{$Z \in [0, 1]$}. 
	\item[$U$]	\textbf{U}pdated prediction of the premium.
\end{description}
\end{distributions}

\begin{distributions}[Terminology]
\begin{description}
	\item[Risk group]	block of insurance policies, covered for a period of time upon payment of a \textit{premium}.
	\item[Claim frequency]	The number of claims denoted $N$.
	\item[Claim severity]	The amount of the $i^{\text{th}}$ claim denoted $X_{i}$.
	\item[Aggregate loss]	The total loss denoted $S$ where $S = X_{1} + X_{2} + \hdots + X_{N}$.
	\item[Pure premium]	The pure premium denoted $P$ where $P = S/E$ with $E$ denoting the number of exposure units.
\end{description}
\end{distributions}

\begin{definitionGENERAL}{Exam tips}[][ao(english)]
Typical questions about this involve being given 3 of $M, D, Z, \text{ and } U$ then finding the missing one.
\end{definitionGENERAL}


\begin{rappel_enhanced}[Context]
With $\min\{D, M\} \leq U \leq \max\{D, M\}$, we can see that the credibility factor determines the relative importance of the claim experience of the risk group $D$ relative to the manual rate $M$.

\bigskip

If $Z = 1$, we obtain \textit{\underline{\nameref{subsec:FullCred}}} where the predicted premium depends only on the data ($U = D$). It follows that with $Z < 1$, we obtain \textit{\underline{\nameref{subsec:PartialCred}}} as the weighted average of both $D$ and $M$.
\end{rappel_enhanced}


%\columnbreak
\subsection{Full Credibility}\label{subsec:FullCred}
\begin{rappel_enhanced}[Contexte]
The classical credibility approach determines the \textit{\textbf{minimum} data size} required for the experience data ($D$) to be given \textbf{\textit{full credibility}}. The minimum data size, or \textit{\textbf{standard for full credibility}}, depends on the \textbf{loss measure}.
\end{rappel_enhanced}


\subsubsection{Claim Frequency}
The claim frequency random variable $N$ has mean $\mu_{N}$ and variance $\sigma^{2}_{N}$. 

If we assume $N \approx \mathcal{N}(\mu_{N}, \sigma^{2}_{N})$, then the probability of observing claim frequency \textbf{within $k$ of the mean} is \lfbox[formula]{$\Pr(\mu_{N} - k\mu_{N} \leq N \leq \mu_{N} + k\mu_{N}) = 2 \Phi\left(\frac{k \mu_{N}}{\sigma_{N}}\right) - 1$}.

\bigskip

We often assume that the claim frequency \lfbox[conditions]{$N \sim \text{Pois}(\lambda_{N})$} and then apply the normal approximation to find the standard for full credibility for claim frequency \lfbox[formula]{$\lambda_{F}$}. First, we impose that the probability of the claim being with $k$ of the mean must be at least $1 - \alpha$. Then, we rewrite \lfbox[conditions]{$\frac{k \mu_{N}}{\sigma_{N}} = k\sqrt{\lambda_{N}}$} and set \lfbox[formula]{$\lambda_{N} \geq \left(\frac{z_{1 - \alpha/2}}{k}\right)^{2}$} where \lfbox[conditions]{$\lambda_{F} = \left(\frac{z_{1 - \alpha/2}}{k}\right)^{2}$}.


\subsubsection{Claim Severity}
We assume that the loss amounts $X_{1}, X_{2}, \dots, X_{N}$ are independent and identically distributed random variables with mean $\mu_{X}$ and variance $\sigma^{2}_{X}$. Full credibility is attributed to \lfbox[conditions]{$D = \bar{X}$} if \lfbox[conditions]{$2\Phi\left(\frac{k \mu_{X}}{\sigma_{N}/\sqrt{N}}\right) - 1 \geq 1 - \alpha$}. 

\bigskip

Similarly to claim frequency, we apply the normal approximation with \lfbox[conditions]{$\bar{X} \approx \mathcal{N}\left(\mu_{X}, \sigma^{2}_{X}/N\right)$}. Then, we find \lfbox[formula]{$N \geq \left(\frac{z_{1 - \alpha/2}}{k}\right)^{2} \cdot \left(\frac{\sigma_{X}}{\mu_{X}}\right)^{2} = \lambda_{F} CV_{X}^{2}$} where the \textbf{\textit{standard for full credibility for claim severity}} is $\lambda_{F}CV_{X}^{2}$.


\subsubsection{Aggregate Loss}
For the aggregate loss $S = X_{1} + X_{2} + \hdots + X_{N}$, we have \lfbox[formula]{$\mu_{S} = \mu_{N} \mu_{X}$} and \lfbox[formula]{$\sigma^{2}_{S} = \mu_{N} \sigma^{2}_{X} + \mu_{X}^{2} \sigma^{2}_{N}$}.

\bigskip

With the same normality assumptions for the Poisson distributed $N$, we find \lfbox[formula]{$\lambda_{N} \geq \left(\frac{z_{1 - \alpha/2}}{k}\right)^{2} \cdot \left(\frac{\mu_{X}^{2} + \sigma^{2}_{X}}{\mu_{X}^{2}}\right) = \lambda_{F} (1 + CV_{X}^{2})$} where the \textbf{\textit{standard for full credibility for claim severity}} is $\lambda_{F}(1 + CV_{X}^{2})$.

\paragraph{Note}	The conditions are the same for the \textit{\textbf{Pure Premium}} as for the aggregate loss.


%\columnbreak
\subsection{Partial Credibility}\label{subsec:PartialCred}
The \textbf{\textit{credibility factor}} for :
\begin{description}
	\item[Claim Frequency]	is \lfbox[formula]{$Z = \sqrt{\frac{\lambda_{N}}{\lambda_{F}}}$}.
	\item[Claim Severity]	is \lfbox[formula]{$Z = \sqrt{\frac{N}{\lambda_{F} CV_{X}^{2}}}$}.
	\item[Aggregate Loss and Pure Premium]	is \lfbox[formula]{$Z = \sqrt{\frac{\lambda_{N}}{\lambda_{F}(1 + CV_{X}^{2})}}$}
\end{description}


\newpage
\section{Bühlmann Credibility}\label{sec:ABuhl}
\begin{rappel_enhanced}[Context]
Buhlmann's approach, a.k.a. the greatest accuracy approach or the least squares approach, estimates the future loss measure $X_{n }$
\end{rappel_enhanced}
\subsection{Basic framework}

 

\subsection{Variance components}



\subsection{Credibility factors}
%Bûhlmann and Bûhlmann-Straub factors
%Estimate loss



\newpage
\section{Bayesian Credibility}\label{sec:ABayes}
\subsection{Basic framework}


\subsection{Premium}



\subsection{Conjugate distributions}

\subsection{Nonparametric empirical Bayes method}




\newpage
\part{Linear Mixed Models}\label{part:LMM}
\begin{rappel_enhanced}[Context]
What distinguishes a linear mixed model is that it may include both \textbf{fixed-effect parameters} and \textbf{random effects}. The mix of these gives the linear \textit{mixed} model its name. Fixed-effect parameters describes the relationships of the covariates to the dependant variable for an \textit{entire population}. Random effects are specific to clusters or subjects \textit{within a population}. Random effects are thus directly used in modelling the random variation in the dependant variable at \underline{different levels} of the data.

\bigskip

Fixed factors are categorical or classification variables for which all levels (conditions) that are of interest are included. Random factors can be thought of as being \textit{randomly sampled} from a population of levels being studied. The text gives as an example the Dental Veneer case study where if we specified the tooth being sampled, selected teeth would become a fixed factor. This would however limit inferences by teeth rather than generalizing to "teeth within a patient". 
\end{rappel_enhanced}

The case studies use 3 types of data:
\begin{description}
	\item[clustered]	The dependant variable is measured once per subject (unit of analysis), and the units are grouped into/nested within clusters of units. 
		\begin{itemize}
		\item	We can have data sets that are two-level (e.g. rat pup data set), three-level (e.g. classroom data), etc.
		\item	For MAS-II, we shouldn't have beyond three levels.
		\end{itemize}
	\item[repeated-measures]	The dependant variable is measured more than once (on the same unit of analysis) across levels of a repeated-measures factor(s). (e.g. time, measurement conditions, etc.)
	\item[longitudinal]	The dependant variable is measured at several points in time for each unit of analysis.
		\begin{itemize}
		\item	\textit{\textbf{Clustered longitudinal}} data combines features of both. (e.g. Dental Veneer data set).
		\item	Each unit is measured more than once, but those units of analysis are nested within clusters.
		\end{itemize}
\end{description}


\begin{rappel_enhanced}[Context]
These 3 are \textbf{\textit{hierarchical}} data sets as the observations can be placed into levels of a hierarchy in the data.

\bigskip

Generally:
\begin{description}
	\item[Level 1]	most detailed level; subjects, repeated measures on the same unit of analysis.
	\item[Level 2]	clusters of units, units of analysis.
	\item[Level 3]	clusters of clusters, clusters of units.
\end{description}

\bigskip

Levels are emphasized in the text because they help to conceptualize LMM as simple models defined at each level of the data hierarchy. 
\end{rappel_enhanced}



\section{General Theory}\label{sec:BGenTheory}
\subsection{Model Assumptions}
\begin{definitionGENERAL}{Fixed Effects}[\circled{1}{trueblue}]

\end{definitionGENERAL}

\begin{definitionGENERAL}{Random Effects}[\circled{2}{trueblue}]

\end{definitionGENERAL}


\subsection{Model specification}
\begin{align*}
	\bm{Y}_{i}
	&=	\underbrace{\bm{X}_{i}\bm{\beta}}_{\text{fixed}} + \underbrace{\bm{Z}_{i}\bm{u}_{i}}_{\text{random}} + \bm{\varepsilon}_{i}
\end{align*}
where:
\begin{itemize}
	\item	$\bm{Y}_{i}$ is the $n_{i} \times 1$ vector of continuous responses for the $i$-th subject.
	\item	$\bm{X}_{i}$ is the $n_{i} \times p$ design matrix.
	\item	$\bm{Z}_{i}$ is the $n_{i} \times q$ design matrix.
		\begin{itemize}
		\item	In a LMM in which only the intercepts are assumed to vary randomly from subject to subject, the $\bm{Z}_{i}$ matrix would simply be a column of $1$'s.
		\end{itemize}	
	\item	$\bm{u}_{i}$ is the $q_{i} \times 1$ matrix (vector) of random effects.
	\item	$\bm{D}$ $q \times q$ variance-covariance matrix of $\bm{u}_{i}$.
	\item	$\bm{\varepsilon}_{i}$ is the $n_{i} \times 1$ matrix (vector) of random effects.
	\item	$\bm{R}_{i}$ $n_{i} \times n_{i}$ variance-covariance matrix of $\bm{\varepsilon}_{i}$.
\end{itemize}

\paragraph{Note}	When LMMs are specified in terms of an explicitly defined hierarchy of simpler models, they are often referred to as hierarchical linear models (HLMs) or multilevel models (MLMs). 

\paragraph{Note}	The elements of both $\bm{R}_{i}$ and $\bm{D}$ are defined as functions of another set of covariance parameters $\bm{\theta}$.

\subsubsection{Random Covariance Structures}
\begin{description}
	\item[unstructured]	$\bm{D}$ matrix with no additional constraints on the values of its elements.
		\begin{itemize}
		\item	Often used for random coefficient models.
		\end{itemize}
	\item[diagonal]	Each random effect $\bm{u}_{i}$ has its own variance, and all covariances in $\bm{D}$ are assumed to be zero.
\end{description}


\subsubsection{Residual Covariance Structures}
\textbf{Diagonal}
\begin{align*}
	R_{i}
	&=	\begin{bmatrix}
		\sigma^{2}	&	0	&	\dots	&	0	\\
		0	&	\sigma^{2}	&	\dots	&	0	\\
		\vdots	&	\vdots	&	\ddots	&	\vdots	\\
		0	&	0	&	\dots	&	\sigma^{2}	\\
		\end{bmatrix}
\end{align*}
\begin{itemize}
	\item	Assumes residuals from the same subject are \textit{uncorrelated} with equal variance.
	\item	Often the default structure.	
\end{itemize}


\textbf{Compound Symmetry}
\begin{align*}
	R_{i}
	&=	\begin{bmatrix}
		\sigma^{2} + \sigma_{1}	&	\sigma_{1}	&	\dots	&	\sigma_{1}	\\
		\sigma_{1}	&	\sigma^{2} + \sigma_{1}	&	\dots	&	\sigma_{1}	\\
		\vdots	&	\vdots	&	\ddots	&	\vdots	\\
		\sigma_{1}	&	\sigma_{1}	&	\dots	&	\sigma^{2} + \sigma_{1}	\\
		\end{bmatrix}
\end{align*}
\begin{itemize}
	\item	Assumes \textit{equal correlation} between residuals from the same individual. (e.g. repeated trials under the same condition in an experiment).
	\item	Good for clustered or repeated measures data.
\end{itemize}


\textbf{First Order Auto-Regressive} ($AR(1)$)
\begin{align*}
	R_{i}
	&=	\begin{bmatrix}
		\sigma^{2}	&	\sigma^{2}\rho	&	\dots	&	\sigma^{2}\rho^{n_{i}-1}	\\
		\sigma^{2}\rho	&	\sigma^{2}	&	\dots	&	\sigma^{2}\rho^{n_{i}-2}	\\
		\vdots	&	\vdots	&	\ddots	&	\vdots	\\
		\sigma^{2}\rho^{n_{i}-1}	&	\sigma^{2}\rho^{n_{i}-2}	&	\dots	&	\sigma^{2}	\\
		\end{bmatrix}
\end{align*}
\begin{itemize}
	\item	Means adjacent residuals have a covariance of $\sigma^{2}\rho$.
	\item	Good for longitudinal observations with \textit{equal time} between observations.
	\item	Implies observations closer to each other have a higher correlation than those that are further apart in time.
\end{itemize}

\paragraph{Note}	We can allow \textbf{heterogeneous variances} for different groups of subjects (e.g. males and females). We could assume the same structure but with different values.


\subsection{The Marginal Linear Model}
\begin{rappel_enhanced}[Context]
Random effects are explicitly used in LMMs to explain the between-subject (between-cluster) variation, but they're not used in specifying marginal models. We thus refer to LMMs as \textit{subject-specific} models, and marginal models as \textit{population-averaged} models.
\end{rappel_enhanced}

We specify the model as $\bm{Y}_{i} = \underbrace{\bm{X}_{i}\bm{\beta}}_{\text{fixed}} + \bm{\varepsilon}_{i}$ where the \textbf{\textit{marginal residual errors}} $\bm{\varepsilon}_{i} \sim \mathcal{N}(0, \bm{V}^{*}_{i})$. 

\bigskip

The big difference here is that the entire random part of the marginal model is described in terms of the marginal residuals $\bm{\varepsilon}_{i}^{*}$ only.

\paragraph{Note}	All structures used for $\bm{R}_{i}$ can be used for $\bm{V}_{i}^{*}$. There are others that can be used, such as the one defined for the implied marginal model. 


\subsubsection{The Implied Marginal Model}
\begin{rappel_enhanced}[Context]
The concept of the implied marginal model is important for at least 2 reasons specified in the text:
\begin{enumerate}
	\item	The framework of the implied marginal model is used to estimate the fixed-effect and covariance parameters in the LMM.
	\item	When we obtain an invalid estimate of the $\bm{D}$ matrix from a software, we can try to fit the implied marginal model (with fewer restrictions) to potentially diagnose problems with nonpositive-definiteness.
\end{enumerate}
\end{rappel_enhanced}

The marginal model implied by the LMM has the variance-covariance matrix $\bm{V}_{i} = \bm{Z}_{i}\bm{D}\bm{Z}_{i}' + \bm{R}_{i}$. 
\begin{itemize}
	\item	Both the LMM and the implied marginal model involve the same set of covariance parameters $\bm{\theta}$, however there are more restrictions imposed on it in the LMM.
	\item	Whereas both $\bm{R}_{i}$ and $\bm{D}$ have to be positive-definite, only $\bm{V}_{i}$ has to be for the implied marginal model.
\end{itemize}

\section{Estimation}
When $\bm{\theta}$ is assumed to be known, we apply the method of \textbf{generalized least squares} (GLS) to estimate $\bm{\beta}$ with MLE. The estimate $\hat{\beta}$ is the \textit{\textbf{best linear unbiased estimator}} (BLUE) of $\bm{\beta}$. The \textbf{\textit{Empirical Best Linear Unbiased Estimator}} (EBLUE) of $\bm{\beta}$ replaces $\bm{V}_{i}$ by its estimate $\hat{\bm{V}}_{i}$.

\bigskip

We obtain that \lfbox[formula]{$\displaystyle \text{Var}(\hat{\bm{\beta}}) = \left(\sum_{i} \bm{X}_{i}'\hat{\bm{V}}_{i}^{-1}\bm{X}_{i}\right)^{-1}$}.

\subsection{REML estimation}
REML is often preferred to ML estimation because it produces unbiased estimates of covariance parameters. It does this by taking into account the loss of df that results from estimating the fixed effects in $\bm{\beta}$.

\bigskip

The resulting estimated $\hat{\bm{\beta}}$ and $\text{Var}(\hat{\bm{\beta}})$ differ because $\hat{\bm{V}}_{i}$ is different. 

\begin{rappel_enhanced}[Context]
The variance of the estimated fixed effects (diagonal elements of $\text{Var}(\hat{\bm{\beta}})$) are biased downward in both ML and REML estimation. This is because neither methods takes into account the uncertainty introduced replacing $\bm{V}_{i}$ by $\hat{\bm{V}}_{i}$.

\bigskip

It follows that $\text{se}(\hat{\bm{\beta}})$ is also biased downward.
\end{rappel_enhanced}


\columnbreak
\section{Algorithms}\label{sec:BAlgorithms}
\begin{definitionNOHFILL}[Expectation Maximization]
The underlying assumption behind the EM algorithm is that optimization of the complete data log-likelihood function is simpler than optimization of the likelihood based on the observed data.

\bigskip

Its main drawback is its slow rate of convergence. Also, the precision of the estimators is overly optimistic. This because the estimators are based on the likelihood from the last maximization step which \hl{uses complete data instead of observed data}. 

\bigskip

Usually used to provide starting values for other algorithms.
\end{definitionNOHFILL}


\begin{definitionNOHFILL}[Newton-Raphson]
Most commonly used in ML and REML estimation of LMMs.

\bigskip

While iterations are more time consuming (given Hessian matrix calculations), but convergence is quicker than the EM algorithm. Another advantage is that the last iteration's Hessian matrix can be used to obtain an asymptotic variance-covariance matrix for the estimated covariance parameters $\bm{\theta}$ to calculate the $\text{se}(\hat{\bm{\theta}})$.
\end{definitionNOHFILL}


\begin{definitionNOHFILL}[Fisher Scoring Algorithm]
Modification of the N-R that uses the \textit{\textbf{expected}} Hassian matrix rather than the observed one. 

\bigskip

Its advantages are that it's more stable numerically, more likely to converge, and has simpler calculations at each iteration. However, it is not recommended to obtain the final estimates. It's primary disadvantage is that it may be difficult to determine the expected value of the Hessian matrix owing to difficulties identifying the appropriate sampling distribution.
\end{definitionNOHFILL}



\subsection{Troubleshooting}
If there are problems fitting the model, these are the steps you can take:
\begin{enumerate}
	\item	Choose alternative starting values for covariance parameter estimates
	\item	Rescale the covariates
		\begin{itemize}
		\item	Improves numerical stability of the optimization algorithm and may circumvent convergence problems.
		\end{itemize}
	\item	Simply and remove some random effects
		\begin{itemize}
		\item	Generally, start with higher-order terms.
		\item	Though, it can be valid to remove lower-order terms, but requires thorough justification.
		\end{itemize}
	\item	Fit the implied marginal model
	\item	Fit the marginal model with an unstructured covariance matrix
\end{enumerate}

Methods 4 and 5 shift a more restrictive requirement for the $\bm{D}$ and $\bm{R}_{i}$ matrices to be positive-definite to a less restrictive requirement that $\bm{V}_{i}$ be positive-definite.

\columnbreak
\section{Hypothesis Testing}\label{sec:BHypTesting}

\subsection{Likelihood Ratio Tests}\label{subsec:LRT}
Mixture of Chi Squares

REML


\subsection{Non-Likelihood Ratio Tests}
\begin{definitionNOHFILL}[$t$-test]

We compensate for the fact that the standard error is underestimated with REML/ML estimation by using an estimated number of df. We approximate the df hence is is not simply $n-p$.
\end{definitionNOHFILL}


\begin{definitionNOHFILL}[$F$-test]
To test $\mathcal{H}_{0}: \bm{L}\bm{\beta} = 0$ vs $\mathcal{H}_{1}: \bm{L}\bm{\beta} \neq 0$.  

\begin{rappel_enhanced}[Context]
Degrees of freedom of the numerator correspond
\end{rappel_enhanced}

We get that the test statistic $t \approx F_{\text{num. df}, \text{den. df}}$ where the numerator df corresponds to the number of parameters being tested and the denominator degrees of freedom is obtained from R. 

\bigskip

The particularity of the $F$-test is that we must make adjustments for it due to:
\begin{enumerate}
	\item	Random Effects
	\item	Potential correlation between residuals
	\item	Estimate covariance matrix
\end{enumerate}

We have a few ways of approximating them:

\begin{definitionNOHFILLpropos}[Scatterwhite]

\begin{itemize}
	\item	Method used by R.
\end{itemize}
\end{definitionNOHFILLpropos}

\begin{definitionNOHFILLpropos}[Kenward-Rogers]

\begin{itemize}
	\item	Method used by SAS.
\end{itemize}
\end{definitionNOHFILLpropos}

\begin{definitionNOHFILLsub}[Type $I$]
Sequential
\end{definitionNOHFILLsub}

\begin{definitionNOHFILLsub}[Type $III$]
Conditional
\end{definitionNOHFILLsub}
\end{definitionNOHFILL}


Using tests:
\begin{enumerate}
	\item	Compute test statistic
		\begin{itemize}
		\item	$F$-statistic would be too hard to compute, would have to be provided.
		\item	For $t$-test, may just give components of the calculation and have us compute $t$ to the compare it to the CV.
		\item	For both tests, the number of df would have to be provided.
		\end{itemize}
	\item	Look up critical value table
	\item	Reject null / keep effects if test statistic > CV
\end{enumerate}

\subsubsection{Other tests}
Omnibus Wald Test (good)
	similar to $F$-test.
	Numerator of the $F$-test statistic.
	test statistic asymptotically $\chi^{2}$

Wald $z$-test (not good)
	only good asymptotically and breaks in some situations
	text recommends LRT instead	
	
	
\section{Model-Building Strategies}
\begin{definitionGENERAL}{Top-Down Strategy}[\circled{1}{trueblue}]
Start with a model that includes the maximum number of fixed effects that we wish to consider. The steps to build the \textit{loaded} mean structure are:

\begin{enumerate}
	\item	Start with a \textbf{well-specified mean structure} for the model
		\begin{itemize}
		\item	Want to ensure that the systematic variation in the responses is well explained before investigating various covariance structures to describe random variation in the data.
		\end{itemize}
	\item	Select a structure for the random effects in the model
		\begin{itemize}
		\item	Selecting set of random effects to include in the model with REML-based LRT.
		\end{itemize}
	\item	Select a covariance structure for the residuals in the model
		\begin{itemize}
		\item	Variation remaining after both fixed and random effects have been added to the model due to residual error.
		\item	Investigate an appropriate covariance structure for the those residuals.
		\end{itemize}
	\item	Reduce the model
		\begin{itemize}
		\item	Use appropriate statistical tests to determine whether certain fixed-effect parameters are needed in the model.
		\end{itemize}
\end{enumerate}
\end{definitionGENERAL}


\begin{definitionGENERAL}{Step-Up Strategy}[\circled{2}{trueblue}]
\begin{enumerate}
	\item	Start with an "unconditional" (or means-only) Level 1 model for the data
	\item	Build the model by adding Level 1 covariates to the Level 1 model. In the Level 2 model, consider adding random effects to the equations for the coefficients of the Level 1 covariates.
	\item	Build the model by adding Level 2 covariates to the Level 2 model. For 3-level models, consider adding random effects to the Level 3 equations for the coefficients of the Level 2 covariates.	
\end{enumerate}
\end{definitionGENERAL}

\section{Checking Model Assumptions}
In general, raw conditional residuals in their basic form are not well suited for verifying model assumptions and detecting outliers. They tend to be correlated and their variances may be different for different subgroups of individuals.

\bigskip

Standardized residuals are conditional residuals divided by their true standard deviations. Unfortunately, the true SDs are rarely knonw in practice and we use estimated SDs instead to obtain studentized residuals.

\bigskip

Scaling the residuals by dividing them by the estimated SD of the dependent variable produces Pearson residuals. This is appropriate when we assume we can ignore the variability of $\hat{\bm{\beta}}$. 

\bigskip

internal vs external studentization.

\bigskip

Influence diagnostics are formal techniques to identify observations that heavily influence estimates of the parameters in either $\bm{\beta}$ or $\bm{\theta}$.

\section{EBLUPS}

\subsection{Intra Correlation Coefficient}
\lfbox[formula]{$ICC_{\text{whatever}} = \frac{\text{variance in common}}{\text{total variance}}$}.

2 level model $ICC_{\text{group}} = \frac{\sigma^{2}_{\text{lvl 2}}}{\sigma^{2}_{\text{lvl 2}} + \sigma^{2}}$
3 level model
	$ICC_{\text{lvl 3 group}} = \frac{\sigma^{2}_{\text{lvl 3}}}{\sigma^{2}_{\text{lvl 3}} + \sigma^{2}_{\text{lvl 2}} + \sigma^{2}}$
	$ICC_{\text{lvl 2 group}} = \frac{\sigma^{2}_{\text{lvl 3}} + \sigma^{2}_{\text{lvl 2}}}{\sigma^{2}_{\text{lvl 3}} + \sigma^{2}_{\text{lvl 2}} + \sigma^{2}}$


\subsection{EBLUPS}
EBLUP
\begin{description}
	\item[E]	
	\item[B]	Best i.e. lowest variance among all such unbiased estimators
	\item[L]	Linear as functions of $\bm{y}_{i}$
	\item[U]	Unbiased with $\text{E}[\hat{\bm{u}}_{i}] = \bm{u}_{i}$
	\item[P]	
\end{description}

\begin{itemize}
	\item	Typically tedious to calculate so we use computers unless we calculate only for 1 random effect.
\end{itemize}


Use Buhlmann's formula where:
\begin{description}
	\item[$M$]	Average predicted value from the implied marginal model
	\item[$\bar{Y}$]	Average observed value from group
	\item[$\sigma^{2}_{HM}$]	$\text{Var}(u_{j}) = \sigma^{2}_{int}$
	\item[$\mu_{PV}$]	$\text{Var}(\varepsilon_{ij}) = \sigma^{2}$
\end{description}
Prediction is for $M + u_{j} = M + Z_{j} (\bar{Y} - M)$.



\section{Information Criteria}
\begin{rappel_enhanced}[Context]
When comparing 2 nested models, the more complex will be better than the simpler model. While the \textit{\underline{\nameref{subsec:LRT}}} checks if the simpler model is sufficient, it does not enable us to directly compare the 2 models. In addition, with the LRT we are limited to nested models. The AIC and BIC measures permit us to compare several models which don't have to be nested. They do so by adding a penalty to the likelihood for a model's complexity via the amount of parameters it has.

\bigskip

We wish to maximize the likelihood of our observations. As observed for the LRT, maximizing the likelihood is equivalent to minimizing the loglikelihood or a function thereof. Namely, $-2 \times \ell(\theta)$ (a.k.a. the \hl{deviance}, see \textit{\underline{\nameref{part:BAandMCMC}}}). In both cases, we add a penalty to the measure we wish to minimize.
\end{rappel_enhanced}

\begin{definitionNOHFILLsub}[Akaike Information Criteria (AIC)]
The AIC penalizes models which have more parameters by adding twice the number of estimated parameters $p$ in the model to twice the negative loglikelihood: \lfbox[formula]{$AIC = -2\ell(\theta) + 2p$}. 

\bigskip

We choose the model with the smallest AIC.
\end{definitionNOHFILLsub}

\begin{rappel_enhanced}[Context]
The disadvantage of the AIC lies in that for 2 nested models the probability of choosing the simpler model knowing it's the true model does not tend towards 1 when the number of observations increases towards infinity. We thus consider it an \textit{inconsistent} measure.

\bigskip

In comparison, the BIC \textbf{is} a \textit{consistent} measure given its parameters penalty is a function of the number of observations.

\bigskip

That being said, in both cases, the probability of rejecting the simpler model while the true model is somewhere in between tends towards 1.
\end{rappel_enhanced}


\begin{definitionNOHFILLsub}[Bayesian Information Criteria (BIC)]
The BIC penalizes more severely models which have more parameters given its penalty is a function of the number of observations $n$: \lfbox[formula]{$BIC = -2\ell(\theta) + \ln(n)p$}.
\end{definitionNOHFILLsub}

\bigskip

To better understand the difference between the AIC and BIC penalty, we can use log rules to rewrite the measures:
\begin{align*}
	AIC	
	&=	-2\ln|\mathcal{L}(\theta)| + 2p	\\
	&=	-2\ln|\mathcal{L}(\theta)| + \ln\left(\textrm{e}^{2p}\right)	\\
	&=	-\left[\ln|\mathcal{L}(\theta)^{2}| - \ln\left|\left(\textrm{e}^{p}\right)^{2}\right|\right]	\\
	&=	-\ln\left|\frac{\mathcal{L}(\theta)^{2}}{\left(\textrm{e}^{p}\right)^{2}}\right|	
\end{align*}

\begin{align*}
	BIC
	&=	-2\ln|\mathcal{L}(\theta)| + \ln|n|p	\\
	&=	-\left[\ln\left|\mathcal{L}(\theta)^{2}\right| - \ln\left|n^{p}\right|\right]	\\
	&=	-\ln\left|\frac{\mathcal{L}(\theta)^{2}}{n^{p}}\right|	
\end{align*}

\bigskip

\begin{rappel_enhanced}[Context]
There's no agreement on which is better for LMM and the text tends to build models piecewise, testing between steps with LRT. So, we probably won't use them much.

\bigskip

Fundamentally, the AIC tries to to find the model that best describes the data under the belief that there is no "correct" model. In contrast, the BIC tries to find the "correct" model under the belief that such a model exists. 

\bigskip

Intuitively, we may think we'd prefer the AIC given that it's typically unrealistic to believe there exists a "correct" model. However, some feel the BIC often gives better results. \textit{However}, part C on \textit{\underline{\nameref{part:BAandMCMC}}} has other information criterion that are more complicated but arguably better. 
\end{rappel_enhanced}


\bigskip

Notes:
\begin{itemize}
	\item	REML criterion at convergence is the deviance ($-2\ell(\theta)$).
		\begin{itemize}
		\item	Likelihoods are typically $<1$ given they're probability densities.
		\item	Thus, loglikelihoods are typically negative.
		\item	Thus a positive output suggests they already multiplied by $-2$.
		\end{itemize}
\end{itemize}



\section{Checking model assumptions}
The following displacement diagnostic measures serve to measure the overall influence of the subset $U$.

\begin{definitionNOHFILLsub}[Likelihood distance / displacement]
The change in Maximum log-Likelihood for all the data with the parameter of interest \lfbox[conditions]{$\bm{\psi}$} estimated with all vs reduced data is \lfbox[formula]{$LD_{(u)} = 2\left(\ell(\hat{\bm{\psi}}) - \ell(\hat{\bm{\psi}}_{(u)}) \right)$}.
\end{definitionNOHFILLsub}

\begin{definitionNOHFILLsub}[Restricted likelihood distance / displacement]
The change in Restricted Maximum log-Likelihood for all the data with the parameter of interest \lfbox[conditions]{$\bm{\psi}$} estimated with all vs reduced data is \lfbox[formula]{$LD_{(u)} = 2\left(\ell_{R}(\hat{\bm{\psi}}) - \ell_{R}(\hat{\bm{\psi}}_{(u)}) \right)$}.
\end{definitionNOHFILLsub}

\bigskip

The following diagnostic measures serve to measure the influence of the subset $U$ on the estimation of the parameters.
\begin{definitionNOHFILLsub}[Cook's $D$]
For parameter of interest $\bm{\beta}$ ou $\bm{\theta}$, the scaled change in the entire estimated $\bm{\beta}$ ou $\bm{\theta}$ vector is
\begin{description}
	\item[$D(\bm{\beta}) =$]	\lfbox[formula]{$\left(\hat{\bm{\beta}} - \hat{\bm{\beta}}_{(u)}\right)' \widehat{\text{Var}}\left[\hat{\bm{\beta}}\right]^{-1}\left(\hat{\bm{\beta}} - \hat{\bm{\beta}}_{(u)}\right)/rank(\bm{X})$}.
	\item[$D(\bm{\theta}) =$]	\lfbox[formula]{$\left(\hat{\bm{\theta}} - \hat{\bm{\theta}}_{(u)}\right)' \widehat{\text{Var}}\left[\hat{\bm{\theta}}\right]^{-1}\left(\hat{\bm{\theta}} - \hat{\bm{\theta}}_{(u)}\right)$}.
\end{description}
\end{definitionNOHFILLsub}

\begin{definitionNOHFILLsub}[Multivariate DFFITS Statistic]
For parameter of interest $\bm{\beta}$ ou $\bm{\theta}$ the scaled change in the entire estimated $\bm{\beta}$ ou $\bm{\theta}$ vector, using externalized estimates of $\text{Var}(\hat{\bm{\beta}})$, is
\begin{description}
	\item[$MDFFITS(\bm{\beta}) =$]	\lfbox[formula]{$\left(\hat{\bm{\beta}} - \hat{\bm{\beta}}_{(u)}\right)' \widehat{\text{Var}}\left[\hat{\bm{\beta}}_{(u)}\right]^{-1}\left(\hat{\bm{\beta}} - \hat{\bm{\beta}}_{(u)}\right)/rank(\bm{X})$}.
	\item[$MDFFITS(\bm{\theta}) =$]	\lfbox[formula]{$\left(\hat{\bm{\theta}} - \hat{\bm{\theta}}_{(u)}\right)' \widehat{\text{Var}}\left[\hat{\bm{\theta}}_{(u)}\right]^{-1}\left(\hat{\bm{\theta}} - \hat{\bm{\theta}}_{(u)}\right)$}.
\end{description}
\end{definitionNOHFILLsub}

\bigskip

The following 2 diagnostic measures go a bit further and measure the influence of the subset $U$ on the \textit{precision} of the estimation of the parameters.

\begin{definitionNOHFILLsub}[Trace of covariance matrix]
For parameter of interest $\bm{\beta}$ ou $\bm{\theta}$, the change in precision of estimated $\bm{\beta}$ ou $\bm{\theta}$ vector, based on trace of $\text{Var}(\hat{\bm{\beta}})$, is
\begin{description}
	\item[$COVTRACE(\bm{\beta}) =$]	\lfbox[formula]{$\left|trace(\widehat{\text{Var}}\left[\hat{\bm{\beta}} - \widehat{\text{Var}}\left[\hat{\bm{\beta}}_{(u)}\right]^{-1}\right]^{-1}) - rank(\bm{X})\right|$}.
	\item[$COVTRACE(\bm{\theta}) =$]	\lfbox[formula]{$\left|trace(\widehat{\text{Var}}\left[\hat{\bm{\theta}} - \widehat{\text{Var}}\left[\hat{\bm{\theta}}_{(u)}\right]^{-1}\right]^{-1}) - q\right|$}.
\end{description}
\end{definitionNOHFILLsub}

\begin{definitionNOHFILLsub}[Covariance ratio]
For parameter of interest $\bm{\beta}$ ou $\bm{\theta}$, the change in precision of estimated $\bm{\beta}$ ou $\bm{\theta}$ vector, based on determinant of $\text{Var}(\hat{\bm{\beta}})$, is
\begin{description}
	\item[$COV RATIO(\bm{\beta}) =$]	\lfbox[formula]{$\frac{\widehat{\text{Var}}\left[\hat{\bm{\beta}}_{(u)}\right]}{\widehat{\text{Var}}\left[\hat{\bm{\beta}}\right]}$}.
	\item[$COV RATIO(\bm{\theta}) =$]		\lfbox[formula]{$\frac{\widehat{\text{Var}}\left[\hat{\bm{\theta}}_{(u)}\right]}{\widehat{\text{Var}}\left[\hat{\bm{\theta}}\right]}$}.
\end{description}
\end{definitionNOHFILLsub}

\bigskip

The following diagnostic measures serve to measure the influence of the subset $U$ on the estimation of the parameters.

\begin{definitionNOHFILLsub}[Sum of squared PRESS residuals]
The sum of PRESS residuals calculated by deleting observations in $U$ is \lfbox[formula]{$PRESS_{(u)} = \sum_{i \in u} (\bm{y}_{i} - x'_{i}\hat{\bm{\beta}}_{(u)})$}.
\end{definitionNOHFILLsub}


\section{Graphical Tests}
Not heavily tested.
	Case study will have some graphs and there will be some questions about case study which may need graphs interpretation
	
marginal residual
	residual leftover plugging in estimated fixed effects
	rarely used
	
conditional (textbook) / response (R) / raw (typical) residuals
	residual from estimate of everything
	In LMM, variance of residual $\varepsilon_{ij}$ can vary based on other factors
	So, still not residual we want

	
standardized / normalized residuals
	conditional residual / estimated SD for that residual
	almost always prefer standardized residual
	
important:
\begin{itemize}
	\item	Use residual plots for normality testing
	\item	raw data plots are useless; ignore them.
	\item	Standardized residuals adjust the data so we can tell if a residual is an outlier because it's from a high variance group or because it's really an outlier.
\end{itemize}


implied marginal model is LMM w/o random effects but with same variance structure (var Yij same for both)

marginal model is with just same variance for everything 

\newpage
\part{Bayesian Analysis and Markov Chain Monte Carlo}\label{part:BAandMCMC}
%%%%	labbels not affixing to the parts? but to previous section? investigate



\newpage
\part{Statistical Learning}\label{part:statLearn}
\section{K-Nearest Neighbors}\label{sec:KNN}


\newpage
\section{Decision Trees}
%	building, purpose, pruning
%	bagging, random forests, boosting


\newpage
\section{Principal Components Analysis (PCA)}\label{sec:PCA}
%	purpose and computations
%	interpretation of software outputs	


\newpage
\section{Clustering}
%	purpose and computations
%	interpretation of software outputs





\end{multicols*}
\end{document}
